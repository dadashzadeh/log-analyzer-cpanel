import re
import json
import gzip
import zipfile
import tarfile
import ipaddress
import shutil
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.chart import BarChart, PieChart, Reference
import requests
from user_agents import parse
import socket
import os
from pathlib import Path
import urllib.parse
import base64
import hashlib
from typing import Dict, List, Tuple, Set, Any

class GoogleBotVerifier:
    """Ú©Ù„Ø§Ø³ ØªØ§ÛŒÛŒØ¯ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ú¯ÙˆÚ¯Ù„ Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±Ø³Ù…ÛŒ"""
    
    def __init__(self):
        self.google_ip_ranges = {
            'googlebot': [],
            'special_crawlers': [],
            'user_triggered': [],
            'user_triggered_google': []
        }
        self.load_google_ip_files()
        
    def load_google_ip_files(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ JSON Ú¯ÙˆÚ¯Ù„"""
        json_files = {
            'googlebot': 'googlebot.json',
            'special_crawlers': 'special-crawlers.json',
            'user_triggered': 'user-triggered-fetchers.json',
            'user_triggered_google': 'user-triggered-fetchers-google.json'
        }
        
        for key, filename in json_files.items():
            if os.path.exists(filename):
                try:
                    with open(filename, 'r') as f:
                        data = json.load(f)
                        if 'prefixes' in data:
                            for prefix in data['prefixes']:
                                if 'ipv4Prefix' in prefix:
                                    self.google_ip_ranges[key].append(
                                        ipaddress.IPv4Network(prefix['ipv4Prefix'])
                                    )
                                elif 'ipv6Prefix' in prefix:
                                    self.google_ip_ranges[key].append(
                                        ipaddress.IPv6Network(prefix['ipv6Prefix'])
                                    )
                    print(f"âœ… {filename}: {len(self.google_ip_ranges[key])} Ø±Ù†Ø¬ IP")
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± {filename}: {e}")
    
    def verify_google_bot(self, ip_str, user_agent):
        """ØªØ§ÛŒÛŒØ¯ Ù‡ÙˆÛŒØª Ø¨Ø§Øª Ú¯ÙˆÚ¯Ù„"""
        result = {
            'is_google': False,
            'bot_type': None,
            'verification_method': None,
            'details': {}
        }
        
        ua_lower = user_agent.lower()
        google_ua_patterns = {
            'googlebot': ['googlebot', 'mediapartners-google', 'google-inspectiontool'],
            'adsbot': ['adsbot-google', 'adsbot-google-mobile'],
            'other': ['google-site-verification', 'chrome-lighthouse', 'googleother']
        }
        
        detected_bot = None
        for bot_type, patterns in google_ua_patterns.items():
            if any(pattern in ua_lower for pattern in patterns):
                detected_bot = bot_type
                break
        
        if not detected_bot:
            return result
        
        result['bot_type'] = detected_bot
        
        try:
            ip_obj = ipaddress.ip_address(ip_str)
            
            for range_type, ip_networks in self.google_ip_ranges.items():
                for network in ip_networks:
                    if ip_obj in network:
                        result['is_google'] = True
                        result['verification_method'] = f'Google {range_type} IP range'
                        result['details']['ip_range'] = str(network)
                        break
                if result['is_google']:
                    break
            
            if not result['is_google']:
                try:
                    hostname = socket.gethostbyaddr(ip_str)[0]
                    result['details']['hostname'] = hostname
                    
                    google_dns_patterns = [
                        r'.*\.googlebot\.com$',
                        r'.*\.google\.com$',
                        r'.*\.googleusercontent\.com$'
                    ]
                    
                    for pattern in google_dns_patterns:
                        if re.match(pattern, hostname):
                            forward_ips = socket.gethostbyname_ex(hostname)[2]
                            if ip_str in forward_ips:
                                result['is_google'] = True
                                result['verification_method'] = 'DNS verification'
                                break
                except:
                    pass
        except:
            pass
        
        return result

class AdvancedSecurityAnalyzer:
    """ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø§Ù…Ù†ÛŒØªÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹"""
    
    def __init__(self, log_file_path: str, site_type: str = 'general'):
        self.log_file_path = log_file_path
        self.site_type = site_type.lower()
        self.logs = []
        self.suspicious_ips = set()
        self.critical_ips = set()  # IP Ù‡Ø§ÛŒ Ø¨Ø§ Ø®Ø·Ø± Ø¨Ø§Ù„Ø§
        self.fake_bots = defaultdict(dict)
        self.analysis_results = {}

        # Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
        self.legitimate_bots = {
            'Google': {
                'patterns': ['googlebot', 'adsbot-google', 'mediapartners-google', 'google-inspectiontool'],
                'dns_suffix': ['.googlebot.com', '.google.com', '.googleusercontent.com'],
                'ip_ranges': []
            },
            'Bing': {
                'patterns': ['bingbot', 'msnbot', 'bingpreview'],
                'dns_suffix': ['.search.msn.com'],
                'ip_ranges': []
            },
            'OpenAI': {
                'patterns': ['gptbot', 'oai-searchbot', 'chatgpt-user', 'openai', 'chatgpt'],
                'dns_suffix': ['.openai.com'],
                'ip_ranges': []
            },
            'PerplexityBot': {
                'patterns': ['perplexitybot', 'perplexity'],
                'dns_suffix': ['.perplexity.ai'],
                'ip_ranges': []
            },
            'PerplexityUser': {
                'patterns': ['perplexity-user'],
                'dns_suffix': ['.perplexity.ai'],
                'ip_ranges': []
            },
            'GoogleCloud': {
                'patterns': [],
                'dns_suffix': ['.google.com'],
                'ip_ranges': []
            },
            'Meta': {
                'patterns': ['facebookbot', 'facebookexternalhit', 'meta-externalagent', 'meta-externalfetcher', 'meta-externalads', 'facebookcatalog'],
                'dns_suffix': ['.facebook.com'],
                'ip_ranges': []
            },
            'LinkedIn': {
                'patterns': ['linkedinbot'],
                'dns_suffix': ['.linkedin.com'],
                'ip_ranges': []
            },
            'ByteDance': {
                'patterns': ['bytespider'],
                'dns_suffix': ['.bytedance.com'],
                'ip_ranges': []
            },
            'DuckDuckGo': {
                'patterns': ['duckassistbot'],
                'dns_suffix': ['.duckduckgo.com'],
                'ip_ranges': []
            },
            'Cohere': {
                'patterns': ['cohere-ai'],
                'dns_suffix': ['.cohere.ai'],
                'ip_ranges': []
            },
            'AllenInstitute': {
                'patterns': ['ai2bot'],
                'dns_suffix': ['.allenai.org'],
                'ip_ranges': []
            },
            'CommonCrawl': {
                'patterns': ['ccbot'],
                'dns_suffix': ['.commoncrawl.org'],
                'ip_ranges': []
            },
            'Diffbot': {
                'patterns': ['diffbot'],
                'dns_suffix': ['.diffbot.com'],
                'ip_ranges': []
            },
            'Omgili': {
                'patterns': ['omgili'],
                'dns_suffix': ['.omgili.com'],
                'ip_ranges': []
            },
            'Timpi': {
                'patterns': ['timpi', 'timpipbot'],
                'dns_suffix': ['.timpi.io'],
                'ip_ranges': []
            },
            'YouCom': {
                'patterns': ['youbot'],
                'dns_suffix': ['.you.com'],
                'ip_ranges': []
            },
            'Mistral': {
                'patterns': ['mistralai-user'],
                'dns_suffix': ['.mistral.ai'],
                'ip_ranges': []
            },
            'Amazon': {
                'patterns': ['amazonbot'],
                'dns_suffix': ['.amazon.com'],
                'ip_ranges': []
            },
            'Apple': {
                'patterns': ['applebot', 'applebot-extended'],
                'dns_suffix': ['.apple.com', '.applebot.apple.com'],
                'ip_ranges': []
            },
            'Yandex': {
                'patterns': ['yandexbot'],
                'dns_suffix': ['.yandex.ru', '.yandex.net'],
                'ip_ranges': []
            },
            'Baidu': {
                'patterns': ['baiduspider'],
                'dns_suffix': ['.baidu.com'],
                'ip_ranges': []
            },
            'SemRush': {
                'patterns': ['semrushbot'],
                'dns_suffix': ['.semrush.com'],
                'ip_ranges': []
            },
            'Ahrefs': {
                'patterns': ['ahrefsbot'],
                'dns_suffix': ['.ahrefs.com'],
                'ip_ranges': []
            },
            'Google-Extended': {
                'patterns': ['google-extended'],
                'dns_suffix': ['.google.com'],
                'ip_ranges': []
            },
            'Google-CloudVertexBot': {
                'patterns': ['google-cloudvertexbot'],
                'dns_suffix': ['.google.com'],
                'ip_ranges': []
            },
            'GoogleOther': {
                'patterns': ['googleother'],
                'dns_suffix': ['.google.com'],
                'ip_ranges': []
            }
        }
        

        # Google Bot Verifier
        self.google_verifier = GoogleBotVerifier()
        for range_type, ip_networks in self.google_verifier.google_ip_ranges.items():
            for network in ip_networks:
                self.legitimate_bots['Google']['ip_ranges'].append(str(network))

        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ IPÙ‡Ø§ÛŒ Bing Ø§Ø² ÙØ§ÛŒÙ„ bingbot.json
        bingbot_file = 'bingbot.json'
        if os.path.exists(bingbot_file):
            try:
                with open(bingbot_file, 'r') as f:
                    data = json.load(f)
                    if 'prefixes' in data:
                        for prefix in data['prefixes']:
                            if 'ipv4Prefix' in prefix:
                                self.legitimate_bots['Bing']['ip_ranges'].append(prefix['ipv4Prefix'])
                            elif 'ipv6Prefix' in prefix:
                                self.legitimate_bots['Bing']['ip_ranges'].append(prefix['ipv6Prefix'])
                print(f"âœ… bingbot.json: {len(self.legitimate_bots['Bing']['ip_ranges'])} Ø±Ù†Ø¬ IP")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ bingbot.json: {e}")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ IPÙ‡Ø§ÛŒ OpenAI Ø§Ø² ÙØ§ÛŒÙ„ gptbot.json
        openai_file = 'gptbot.json'
        if os.path.exists(openai_file):
            try:
                with open(openai_file, 'r') as f:
                    data = json.load(f)
                    if 'prefixes' in data:
                        for prefix in data['prefixes']:
                            if 'ipv4Prefix' in prefix:
                                self.legitimate_bots['OpenAI']['ip_ranges'].append(prefix['ipv4Prefix'])
                            elif 'ipv6Prefix' in prefix:
                                self.legitimate_bots['OpenAI']['ip_ranges'].append(prefix['ipv6Prefix'])
                print(f"âœ… gptbot.json: {len(self.legitimate_bots['OpenAI']['ip_ranges'])} Ø±Ù†Ø¬ IP")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ gptbot.json: {e}")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ IPÙ‡Ø§ÛŒ Perplexity Bot Ø§Ø² ÙØ§ÛŒÙ„ perplexitybot.json
        perplexity_bot_file = 'perplexitybot.json'
        if os.path.exists(perplexity_bot_file):
            try:
                with open(perplexity_bot_file, 'r') as f:
                    data = json.load(f)
                    if 'prefixes' in data:
                        for prefix in data['prefixes']:
                            if 'ipv4Prefix' in prefix:
                                self.legitimate_bots['PerplexityBot']['ip_ranges'].append(prefix['ipv4Prefix'])
                            elif 'ipv6Prefix' in prefix:
                                self.legitimate_bots['PerplexityBot']['ip_ranges'].append(prefix['ipv6Prefix'])
                print(f"âœ… perplexitybot.json: {len(self.legitimate_bots['PerplexityBot']['ip_ranges'])} Ø±Ù†Ø¬ IP")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ perplexitybot.json: {e}")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ IPÙ‡Ø§ÛŒ Perplexity User Ø§Ø² ÙØ§ÛŒÙ„ perplexity-user.json
        perplexity_user_file = 'perplexity-user.json'
        if os.path.exists(perplexity_user_file):
            try:
                with open(perplexity_user_file, 'r') as f:
                    data = json.load(f)
                    if 'prefixes' in data:
                        for prefix in data['prefixes']:
                            if 'ipv4Prefix' in prefix:
                                self.legitimate_bots['PerplexityUser']['ip_ranges'].append(prefix['ipv4Prefix'])
                            elif 'ipv6Prefix' in prefix:
                                self.legitimate_bots['PerplexityUser']['ip_ranges'].append(prefix['ipv6Prefix'])
                print(f"âœ… perplexity-user.json: {len(self.legitimate_bots['PerplexityUser']['ip_ranges'])} Ø±Ù†Ø¬ IP")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ perplexity-user.json: {e}")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ IPÙ‡Ø§ÛŒ Google Cloud Ø§Ø² ÙØ§ÛŒÙ„ cloud.json
        cloud_file = 'cloud.json'
        if os.path.exists(cloud_file):
            try:
                with open(cloud_file, 'r') as f:
                    data = json.load(f)
                    if 'prefixes' in data:
                        for prefix in data['prefixes']:
                            if 'ipv4Prefix' in prefix:
                                self.legitimate_bots['GoogleCloud']['ip_ranges'].append(prefix['ipv4Prefix'])
                            elif 'ipv6Prefix' in prefix:
                                self.legitimate_bots['GoogleCloud']['ip_ranges'].append(prefix['ipv6Prefix'])
                print(f"âœ… cloud.json: {len(self.legitimate_bots['GoogleCloud']['ip_ranges'])} Ø±Ù†Ø¬ IP")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ cloud.json: {e}")

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ù…Ù†ÛŒØªÛŒ
        self.security_thresholds = {
            'requests_per_minute': 30,
            'requests_per_hour': 500,
            'failed_login_attempts': 5,
            'error_404_threshold': 20,
            'large_response_size': 1048576,  # 1MB
            'suspicious_score_threshold': 50
        }

        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø­Ù…Ù„Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
        self.advanced_attack_patterns = {
            'sql_injection': {
                'patterns': [
                    r'(union|select|insert|update|delete|drop)[\s\+]+',
                    r'(or|and)[\s\+]*[\'\"\d][\s\+]*=',
                    r'exec(\s|\+|%20)*\((sp_|xp_)',
                    r'(;|%3B)[\s]*(drop|delete|truncate|update)',
                    r'(benchmark|sleep|waitfor[\s]+delay)[\s]*\(',
                    r'(load_file|into[\s]+outfile|into[\s]+dumpfile)',
                    r'concat[\s]*\(.*,.*\)',
                    r'group[\s]+by[\s]+\d+[\s]+having',
                    r'information_schema\.',
                    r'sysobjects|syscolumns|systables'
                ],
                'severity': 'CRITICAL'
            },
            'xss': {
                'patterns': [
                    r'<script[\s>]',
                    r'javascript:',
                    r'on\w+[\s]*=[\s]*["\']',
                    r'<iframe[\s>]',
                    r'<embed[\s>]',
                    r'<object[\s>]',
                    r'document\.(cookie|write|domain|referrer)',
                    r'window\.(location|open)',
                    r'eval[\s]*\(',
                    r'expression[\s]*\(',
                    r'<svg[\s]+on\w+',
                    r'data:text/html',
                    r'vbscript:',
                    r'<img[\s]+src[\s]*=.*on\w+',
                    r'&#x?[0-9a-f]+;'
                ],
                'severity': 'HIGH'
            },
            'lfi_rfi': {
                'patterns': [
                    r'\.\./\.\./\.\./\.\./\.\./etc/passwd',
                    r'\.\.[\\/]',
                    r'%2e%2e[\\/]',
                    r'%252e%252e',
                    r'(include|require|include_once|require_once)[\s]*\(',
                    r'php://input',
                    r'php://filter',
                    r'data://text/plain',
                    r'expect://\w+',
                    r'zip://.*#',
                    r'phar://',
                    r'file:///\w+',
                    r'/proc/self/environ',
                    r'/var/log/\w+',
                    r'c:\\\\windows\\\\',
                    r'c:\\\\winnt\\\\'
                ],
                'severity': 'CRITICAL'
            },
            'command_injection': {
                'patterns': [
                    r';[\s]*(ls|cat|wget|curl|bash|sh)[\s]',
                    r'\|[\s]*(ls|cat|wget|curl|bash|sh)[\s]',
                    r'`(ls|cat|wget|curl|bash|sh)`',
                    r'\$\((ls|cat|wget|curl|bash|sh)\)',
                    r'(ping|nc|netcat|ncat)[\s]+[\d\.]+',
                    r'&[\s]*(ls|cat|wget|curl|bash|sh)[\s]',
                    r'{\$.*}',
                    r'(chmod|chown|chgrp)[\s]+[\d]+',
                    r'(kill|killall|pkill)[\s]+',
                    r'(python|perl|ruby|php)[\s]+-[ce]',
                    r'(whoami|id|uname|hostname)',
                    r'(passwd|shadow|sudoers)',
                    r'/bin/(sh|bash|dash|ksh|tcsh|zsh)'
                ],
                'severity': 'CRITICAL'
            },
            'xxe': {
                'patterns': [
                    r'<!DOCTYPE[^>]*\[<!ENTITY',
                    r'SYSTEM[\s]*["\']file://',
                    r'SYSTEM[\s]*["\']http://',
                    r'SYSTEM[\s]*["\']https://',
                    r'<!ENTITY[\s]+\w+[\s]+SYSTEM',
                    r'&xxe;',
                    r'%xxe;'
                ],
                'severity': 'HIGH'
            },
            'ldap_injection': {
                'patterns': [
                    r'\*\|\(\w+=\*\)',
                    r'\(\|\(\w+=\*\)\)',
                    r'\(\&\(\w+=\*\)\)',
                    r'[\*\(\)\|&=]'
                ],
                'severity': 'MEDIUM'
            },
            'xpath_injection': {
                'patterns': [
                    r'[\'\"][\s]*or[\s]*[\'\"]',
                    r'[\'\"][\s]*and[\s]*[\'\"]',
                    r'[/\*]+',
                    r'count\(\/\/',
                    r'name\(\)'
                ],
                'severity': 'MEDIUM'
            },
            'ssti': {
                'patterns': [
                    r'{{.*}}',
                    r'{%.*%}',
                    r'\${.*}',
                    r'<%.*%>',
                    r'#\{.*\}',
                    r'{{[\s]*config[\s]*}}',
                    r'{{[\s]*request[\s]*}}',
                    r'{{[\s]*self[\s]*}}'
                ],
                'severity': 'HIGH'
            },
            'log4j': {
                'patterns': [
                    r'\$\{jndi:',
                    r'\$\{.*:.*:.*\}',
                    r'jndi:ldap://',
                    r'jndi:rmi://',
                    r'jndi:dns://'
                ],
                'severity': 'CRITICAL'
            },
            'wordpress_specific': {
                'patterns': [
                    r'/wp-admin/admin-ajax\.php.*action=.*',
                    r'/xmlrpc\.php',
                    r'/wp-json/wp/v2/users',
                    r'/wp-content/plugins/.*\.php\?',
                    r'/wp-content/themes/.*\.php\?',
                    r'/wp-content/uploads/.*\.php',
                    r'/wp-config\.php',
                    r'/wp-login\.php.*log=.*pwd=',
                    r'author=\d+',
                    r'/\?author=\d+'
                ],
                'severity': 'MEDIUM'
            },
            'opencart_specific': {
                'patterns': [
                    r'/admin/controller/.*\.php',
                    r'/system/storage/.*',
                    r'/system/library/.*\.php\?',
                    r'/catalog/controller/.*\.php\?',
                    r'/admin/config\.php',
                    r'/index\.php\?route=.*[\'"]',
                    r'/journal2/.*',
                    r'/vqmod/.*\.php'
                ],
                'severity': 'MEDIUM'
            },
            'scanner_detection': {
                'patterns': [
                    r'/\.git/',
                    r'/\.svn/',
                    r'/\.env',
                    r'/\.aws/',
                    r'/\.docker/',
                    r'/backup\.',
                    r'/database\.',
                    r'/db\.',
                    r'/dump\.',
                    r'/(test|demo|dev|staging|backup|old|temp|tmp)/',
                    r'/phpinfo\.php',
                    r'/info\.php',
                    r'/test\.php',
                    r'/robots\.txt',
                    r'/sitemap\.xml',
                    r'/crossdomain\.xml',
                    r'/clientaccesspolicy\.xml',
                    r'/\.well-known/',
                    r'/admin\.php',
                    r'/login\.php',
                    r'/shell\.php',
                    r'/c99\.php',
                    r'/r57\.php',
                    r'/backdoor\.php'
                    r'/upload\.php'
                    r'/cmd\.php'
                ],
                'severity': 'MEDIUM'
            },
            'authentication_bypass': {
                'patterns': [
                    r'admin[\'"\s]*=[\'"\s]*[\'"]',
                    r'[\'"][\s]*or[\s]*1[\s]*=[\s]*1',
                    r'[\'"][\s]*or[\s]*[\'"]1[\'"][\s]*=[\'"]1',
                    r'admin\'--',
                    r'admin"--',
                    r'admin\#',
                    r'[\'\"][\s]*or[\s]*true',
                    r'[\'\"][\s]*or[\s]*[\'\"]x[\'\"][\s]*=[\s]*[\'\"]x'
                ],
                'severity': 'CRITICAL'
            },
            'directory_listing': {
                'patterns': [
                    r'/\?C=[NMSD];O=[AD]',
                    r'/\?dir',
                    r'/\?sort=',
                    r'/%3f(C|O|M|D|N|S)=',
                    r'/index\.php\?path=',
                    r'/index\.php\?dir='
                ],
                'severity': 'LOW'
            },
            'sensitive_files': {
                'patterns': [
                    r'\.(sql|bak|backup|old|orig|original|copy|tmp|temp|log|swp)($|\?)',
                    r'/(id_rsa|id_dsa|id_ecdsa|id_ed25519)($|\.pub)',
                    r'/\.(ssh|bash_history|zsh_history|mysql_history)/',
                    r'/wp-config\.php\.(bak|old|backup|swp)',
                    r'/(config|configuration|database|db)\.(inc|ini|conf|cfg|json|yml|yaml)',
                    r'/\.(htaccess|htpasswd)($|\.)',
                    r'/web\.config($|\.)',
                    r'/(composer|package)\.(json|lock)',
                    r'/Dockerfile',
                    r'/docker-compose\.yml',
                    r'/\.env($|\.)',
                    r'/(private|secret|key|pass|passwd|password)[\w]*\.'
                ],
                'severity': 'HIGH'
            }
        }
        
        # User-Agent Ù‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ©
        self.suspicious_user_agents = {
            'hacking_tools': [
                'nikto', 'sqlmap', 'nmap', 'masscan', 'metasploit',
                'burpsuite', 'burp', 'zaproxy', 'zap', 'acunetix',
                'nessus', 'qualys', 'openvas', 'nexpose', 'appscan',
                'wpscan', 'joomscan', 'droopescan', 'cmsmap',
                'dirbuster', 'dirb', 'gobuster', 'wfuzz', 'ffuf',
                'hydra', 'medusa', 'brutus', 'john', 'hashcat',
                'havij', 'sqlninja', 'pangolin', 'absinthe',
                'beef', 'commix', 'shellshock', 'struct',
                'nuclei', 'jaeles', 'xray', 'rad', 'crawlergo',
                'subfinder', 'httpx', 'naabu', 'meg',
                'python-requests/', 'python-urllib/', 'go-http-client',
                'curl/', 'wget/', 'libwww-perl', 'lwp-trivial'
            ],
            'suspicious_bots': [
                'bot', 'crawler', 'spider', 'scraper', 'scan',
                'fetch', 'check', 'monitor', 'test', 'debug',
                'validator', 'analyzer', 'extractor'
            ],
            'programming_libraries': [
                'python', 'perl', 'ruby', 'java', 'php',
                'node', 'golang', 'rust', 'lua',
                'mechanize', 'scrapy', 'beautifulsoup',
                'httpclient', 'okhttp', 'retrofit'
            ]
        }
        

        # HTTP Methods
        self.suspicious_methods = ['PUT', 'DELETE', 'TRACE', 'CONNECT', 'PATCH', 'OPTIONS']
        
        # Response codes analysis
        self.error_codes = {
            '400': 'Bad Request',
            '401': 'Unauthorized',
            '403': 'Forbidden',
            '404': 'Not Found',
            '405': 'Method Not Allowed',
            '500': 'Internal Server Error',
            '502': 'Bad Gateway',
            '503': 'Service Unavailable'
        }
    
    def extract_file(self) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„ gz/zip/tar.gz Ùˆ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯"""
        file_path = Path(self.log_file_path)
        extracted_files = []

        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ù…ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬
        extract_dir = Path('extracted_logs')
        extract_dir.mkdir(exist_ok=True)

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ tar.gz
        if file_path.suffix == '.gz' and file_path.name.endswith('.tar.gz'):
            print(f"ğŸ“¦ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø±Ø´ÛŒÙˆ tar.gz: {file_path.name}...")

            with tarfile.open(file_path, 'r:gz') as tar:
                # Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÙˆÙ† Ø¢Ø±Ø´ÛŒÙˆ
                members = tar.getmembers()
                log_members = []

                # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯
                for member in members:
                    if member.isfile():
                        name_lower = member.name.lower()
                        if 'access' in name_lower or 'log' in name_lower:
                            log_members.append(member)
                            print(f"  ğŸ“„ ÛŒØ§ÙØª Ø´Ø¯: {member.name}")

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯
                for member in log_members:
                    tar.extract(member, extract_dir)
                    extracted_path = extract_dir / member.name

                    # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ù‡Ù… ÙØ´Ø±Ø¯Ù‡ Ø§Ø³Øª
                    if extracted_path.suffix == '.gz':
                        print(f"    ğŸ“¦ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡: {extracted_path.name}...")
                        decompressed_path = extracted_path.with_suffix('')

                        with gzip.open(extracted_path, 'rb') as gz_file:
                            with open(decompressed_path, 'wb') as out_file:
                                out_file.write(gz_file.read())

                        extracted_files.append(str(decompressed_path))
                        # Ø­Ø°Ù ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡ Ù…ÙˆÙ‚Øª
                        extracted_path.unlink()
                    else:
                        extracted_files.append(str(extracted_path))

            print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ {len(extracted_files)} ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ gz Ù…Ø¹Ù…ÙˆÙ„ÛŒ
        elif file_path.suffix == '.gz':
            print(f"ğŸ“¦ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {file_path.name}...")
            extracted_path = file_path.with_suffix('')

            with gzip.open(file_path, 'rb') as gz_file:
                with open(extracted_path, 'wb') as output_file:
                    content = gz_file.read()
                    output_file.write(content)

            extracted_files.append(str(extracted_path))
            print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {extracted_path.name}")

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ zip
        elif file_path.suffix == '.zip':
            print(f"ğŸ“¦ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {file_path.name}...")

            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                # Ù„ÛŒØ³Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÙˆÙ† zip
                for file_info in zip_ref.filelist:
                    name_lower = file_info.filename.lower()
                    if 'access' in name_lower or 'log' in name_lower:
                        print(f"  ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬: {file_info.filename}")
                        zip_ref.extract(file_info, extract_dir)
                        extracted_path = extract_dir / file_info.filename

                        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ù‡Ù… ÙØ´Ø±Ø¯Ù‡ Ø§Ø³Øª
                        if extracted_path.suffix == '.gz':
                            print(f"    ğŸ“¦ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡: {extracted_path.name}...")
                            decompressed_path = extracted_path.with_suffix('')

                            with gzip.open(extracted_path, 'rb') as gz_file:
                                with open(decompressed_path, 'wb') as out_file:
                                    out_file.write(gz_file.read())

                            extracted_files.append(str(decompressed_path))
                            extracted_path.unlink()
                        else:
                            extracted_files.append(str(extracted_path))

            print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ {len(extracted_files)} ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")

        # ÙØ§ÛŒÙ„ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
        else:
            extracted_files.append(str(file_path))

        return extracted_files if extracted_files else [str(file_path)]

    def parse_log_line(self, line: str) -> Dict:
        """Ù¾Ø§Ø±Ø³ Ø®Ø· Ù„Ø§Ú¯ Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        # Combined Log Format
        pattern = r'(\S+) - - \[(.*?)\] "(.*?)" (\d+) (\d+|-) "(.*?)" "(.*?)"'
        match = re.match(pattern, line)
        
        if match:
            # Ù¾Ø§Ø±Ø³ HTTP method Ùˆ URL
            request = match.group(3)
            request_parts = request.split(' ')
            method = request_parts[0] if len(request_parts) > 0 else 'UNKNOWN'
            url = request_parts[1] if len(request_parts) > 1 else '/'
            protocol = request_parts[2] if len(request_parts) > 2 else 'HTTP/1.0'
            
            # Ù¾Ø§Ø±Ø³ timestamp
            timestamp_str = match.group(2)
            try:
                # Format: "21/Sep/2025:15:42:03 +0330"
                dt = datetime.strptime(timestamp_str.split(' ')[0], '%d/%b/%Y:%H:%M:%S')
            except:
                dt = datetime.now()
            
            return {
                'ip': match.group(1),
                'timestamp': timestamp_str,
                'datetime': dt,
                'request': request,
                'method': method,
                'url': url,
                'protocol': protocol,
                'status_code': int(match.group(4)),
                'bytes': int(match.group(5)) if match.group(5) != '-' else 0,
                'referrer': match.group(6),
                'user_agent': match.group(7)
            }
        
        return None

    def select_time_period(self) -> int:
        """Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„"""
        print("\n" + "="*60)
        print("ğŸ“… Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„:")
        print("-"*60)
        print("1. ÛŒÚ© Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±")
        print("2. Ø¯Ùˆ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±") 
        print("3. Ø³Ù‡ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±")
        print("4. Ø´Ø´ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±")
        print("5. Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±")
        print("6. Ú©Ù„ Ù„Ø§Ú¯â€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø²Ù…Ø§Ù†ÛŒ)")
        print("-"*60)

        while True:
            try:
                choice = input("ğŸ”¢ Ú¯Ø²ÛŒÙ†Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ (1-6): ").strip()
                if choice in ['1', '2', '3', '4', '5', '6']:
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²
                    days_map = {
                        '1': 30,    # 1 Ù…Ø§Ù‡
                        '2': 60,    # 2 Ù…Ø§Ù‡
                        '3': 90,    # 3 Ù…Ø§Ù‡
                        '4': 180,   # 6 Ù…Ø§Ù‡
                        '5': 365,   # 12 Ù…Ø§Ù‡
                        '6': 0      # Ú©Ù„ (Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª)
                    }

                    period_names = {
                        '1': 'ÛŒÚ© Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                        '2': 'Ø¯Ùˆ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                        '3': 'Ø³Ù‡ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                        '4': 'Ø´Ø´ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                        '5': 'Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                        '6': 'Ú©Ù„ Ù„Ø§Ú¯â€ŒÙ‡Ø§'
                    }

                    print(f"\nâœ… Ø¨Ø§Ø²Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡: {period_names[choice]}")
                    return days_map[choice]
                else:
                    print("âŒ Ù„Ø·ÙØ§Ù‹ Ø¹Ø¯Ø¯ÛŒ Ø¨ÛŒÙ† 1 ØªØ§ 6 ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯")
            except KeyboardInterrupt:
                print("\nâš ï¸ Ø¹Ù…Ù„ÛŒØ§Øª Ù„ØºÙˆ Ø´Ø¯")
                sys.exit(0)
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§: {e}")

    def load_logs(self, days_limit: int = 0) -> List[Dict]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø§Ø±Ø³ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† ÙØ§ÛŒÙ„"""
        log_files = self.extract_file()

        print(f"ğŸ“– Ø®ÙˆØ§Ù†Ø¯Ù† {len(log_files)} ÙØ§ÛŒÙ„ Ù„Ø§Ú¯...")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ§Ø±ÛŒØ® cutoff
        cutoff_date = None
        if days_limit > 0:
            cutoff_date = datetime.now() - timedelta(days=days_limit)
            print(f"ğŸ• ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ: Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² {cutoff_date.strftime('%Y-%m-%d')}")

        total_lines = 0
        parsed_lines = 0
        filtered_lines = 0
        file_stats = {}

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± ÙØ§ÛŒÙ„ Ù„Ø§Ú¯
        for log_file in sorted(log_files):
            print(f"\n  ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {Path(log_file).name}")
            file_total = 0
            file_parsed = 0
            file_filtered = 0

            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']

            for encoding in encodings:
                try:
                    with open(log_file, 'r', encoding=encoding, errors='ignore') as f:
                        for line_num, line in enumerate(f, 1):
                            file_total += 1
                            total_lines += 1

                            if total_lines % 10000 == 0:
                                print(f"    Ù¾Ø±Ø¯Ø§Ø²Ø´: {total_lines:,} Ø®Ø·...")

                            parsed = self.parse_log_line(line.strip())
                            if parsed:
                                file_parsed += 1
                                parsed_lines += 1

                                # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø²Ù…Ø§Ù†ÛŒ
                                if cutoff_date:
                                    if parsed['datetime'] < cutoff_date:
                                        file_filtered += 1
                                        filtered_lines += 1
                                        continue
                                    
                                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù…Ù†Ø¨Ø¹
                                parsed['source_file'] = Path(log_file).name
                                self.logs.append(parsed)

                    file_stats[Path(log_file).name] = {
                        'total': file_total,
                        'parsed': file_parsed,
                        'filtered': file_filtered,
                        'loaded': file_parsed - file_filtered
                    }

                    print(f"    âœ“ {Path(log_file).name}:")
                    print(f"      Ú©Ù„ Ø®Ø·ÙˆØ·: {file_total:,}")
                    print(f"      Ù¾Ø§Ø±Ø³ Ø´Ø¯Ù‡: {file_parsed:,}")
                    if days_limit > 0:
                        print(f"      ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡: {file_filtered:,}")
                        print(f"      Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡: {file_parsed - file_filtered:,}")

                    break

                except UnicodeDecodeError:
                    if encoding == encodings[-1]:
                        print(f"    âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {Path(log_file).name}")
                    continue
                
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
        self.logs.sort(key=lambda x: x['datetime'])

        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
        print(f"\nâœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„:")
        print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {len(log_files)}")
        print(f"  â€¢ Ú©Ù„ Ø®Ø·ÙˆØ·: {total_lines:,}")
        print(f"  â€¢ Ù¾Ø§Ø±Ø³ Ø´Ø¯Ù‡: {parsed_lines:,}")
        if days_limit > 0:
            print(f"  â€¢ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ (Ù‚Ø¯ÛŒÙ…ÛŒ): {filtered_lines:,}")
            print(f"  â€¢ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {len(self.logs):,}")
        else:
            print(f"  â€¢ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡: {len(self.logs):,}")
        print(f"  â€¢ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡: {total_lines - parsed_lines:,}")

        # Ù†Ù…Ø§ÛŒØ´ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        if self.logs:
            date_range = {
                'start': min(log['datetime'] for log in self.logs),
                'end': max(log['datetime'] for log in self.logs)
            }
            print(f"  â€¢ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {date_range['start'].strftime('%Y-%m-%d')} ØªØ§ {date_range['end'].strftime('%Y-%m-%d')}")

        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù‡Ø± ÙØ§ÛŒÙ„
        if len(file_stats) > 1:
            print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§:")
            for filename, stats in file_stats.items():
                print(f"  â€¢ {filename}: {stats['loaded']:,} Ù„Ø§Ú¯")

        # Ø­Ø°Ù ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª
        self._cleanup_temp_files()

        return self.logs
    
    def _cleanup_temp_files(self):
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡"""
        extract_dir = Path('extracted_logs')
        if extract_dir.exists():
            try:
                import shutil
                shutil.rmtree(extract_dir)
                print("ğŸ§¹ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù†Ø¯")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª: {e}")

    def calculate_ip_risk_score(self, ip: str) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø±ÛŒØ³Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ø± IP"""
        score = 0
        reasons = []
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ IP
        ip_logs = [log for log in self.logs if log['ip'] == ip]
        
        # 1. ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
        request_count = len(ip_logs)
        if request_count > 1000:
            score += 30
            reasons.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø²ÛŒØ§Ø¯ ({request_count})")
        elif request_count > 500:
            score += 20
            reasons.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…ØªÙˆØ³Ø· ({request_count})")
        elif request_count > 100:
            score += 10
            reasons.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ ({request_count})")
        
        # 2. ØªØ­Ù„ÛŒÙ„ User-Agent
        user_agents = set(log['user_agent'] for log in ip_logs)
        suspicious_ua_count = 0
        
        for ua in user_agents:
            ua_lower = ua.lower()
            # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡Ú©
            for tool in self.suspicious_user_agents['hacking_tools']:
                if tool in ua_lower:
                    score += 25
                    suspicious_ua_count += 1
                    reasons.append(f"Ø§Ø¨Ø²Ø§Ø± Ù‡Ú©: {tool}")
                    break
            
            # User-Agent Ø®Ø§Ù„ÛŒ
            if ua == '-' or len(ua) < 5:
                score += 15
                reasons.append("User-Agent Ø®Ø§Ù„ÛŒ/Ú©ÙˆØªØ§Ù‡")
        
        # 3. HTTP Methods Ù…Ø´Ú©ÙˆÚ©
        methods = Counter(log['method'] for log in ip_logs)
        for method in self.suspicious_methods:
            if method in methods:
                score += 10
                reasons.append(f"HTTP Method Ù…Ø´Ú©ÙˆÚ©: {method}")
        
        # 4. Ú©Ø¯Ù‡Ø§ÛŒ Ø®Ø·Ø§
        status_codes = Counter(log['status_code'] for log in ip_logs)
        error_count = sum(count for code, count in status_codes.items() if code >= 400)
        if error_count > 50:
            score += 20
            reasons.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§ÛŒ Ø²ÛŒØ§Ø¯ ({error_count})")
        elif error_count > 20:
            score += 10
            reasons.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§ÛŒ Ù…ØªÙˆØ³Ø· ({error_count})")
        
        # 5. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø­Ù…Ù„Ù‡
        attack_count = 0
        attack_types = set()
        
        for log in ip_logs:
            request = log['request']
            for attack_type, attack_info in self.advanced_attack_patterns.items():
                for pattern in attack_info['patterns']:
                    if re.search(pattern, request, re.IGNORECASE):
                        attack_count += 1
                        attack_types.add(attack_type)
                        
                        if attack_info['severity'] == 'CRITICAL':
                            score += 30
                        elif attack_info['severity'] == 'HIGH':
                            score += 20
                        elif attack_info['severity'] == 'MEDIUM':
                            score += 10
                        else:
                            score += 5
                        break
        
        if attack_count > 0:
            reasons.append(f"ØªØ¹Ø¯Ø§Ø¯ Ø­Ù…Ù„Ø§Øª: {attack_count} ({', '.join(attack_types)})")
        
        # 6. Ù†Ø±Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øª (requests per minute)
        if ip_logs:
            timestamps = sorted([log['datetime'] for log in ip_logs])
            time_range = (timestamps[-1] - timestamps[0]).total_seconds() / 60  # Ø¯Ù‚ÛŒÙ‚Ù‡
            
            if time_range > 0:
                requests_per_minute = len(ip_logs) / time_range
                if requests_per_minute > 100:
                    score += 30
                    reasons.append(f"Ù†Ø±Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø§Ù„Ø§ ({requests_per_minute:.1f}/min)")
                elif requests_per_minute > 50:
                    score += 20
                    reasons.append(f"Ù†Ø±Ø® Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…ØªÙˆØ³Ø· ({requests_per_minute:.1f}/min)")
        
        # 7. ØªÙ†ÙˆØ¹ URL Ù‡Ø§ (Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø³Ú©Ù†Ø±Ù‡Ø§)
        unique_urls = len(set(log['url'] for log in ip_logs))
        if unique_urls > 100:
            score += 20
            reasons.append(f"Ø§Ø³Ú©Ù† URL ({unique_urls} URL Ù…ØªÙØ§ÙˆØª)")
        elif unique_urls > 50:
            score += 10
            reasons.append(f"ØªÙ†ÙˆØ¹ URL Ø¨Ø§Ù„Ø§ ({unique_urls} URL)")
        
        # 8. Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ø§Ø³
        sensitive_access = 0
        for log in ip_logs:
            for pattern in self.advanced_attack_patterns['sensitive_files']['patterns']:
                if re.search(pattern, log['url'], re.IGNORECASE):
                    sensitive_access += 1
                    score += 10
                    break
        
        if sensitive_access > 0:
            reasons.append(f"Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ø§Ø³ ({sensitive_access} Ù…ÙˆØ±Ø¯)")
        
        # ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ø±ÛŒØ³Ú©
        if score >= 100:
            risk_level = 'CRITICAL'
        elif score >= 70:
            risk_level = 'HIGH'
        elif score >= 40:
            risk_level = 'MEDIUM'
        elif score >= 20:
            risk_level = 'LOW'
        else:
            risk_level = 'SAFE'
        
        return {
            'ip': ip,
            'score': score,
            'risk_level': risk_level,
            'reasons': reasons,
            'request_count': request_count,
            'unique_urls': unique_urls,
            'error_count': error_count
        }
    
    def analyze_temporal_patterns(self) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø­Ù…Ù„Ø§Øª"""
        hourly_stats = defaultdict(int)
        daily_stats = defaultdict(int)
        
        for log in self.logs:
            dt = log['datetime']
            hourly_stats[dt.hour] += 1
            daily_stats[dt.strftime('%Y-%m-%d')] += 1
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³Ø§Ø¹Ø§Øª Ù¾Ø±ØªØ±Ø§ÙÛŒÚ©
        peak_hours = sorted(hourly_stats.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ù¾Ø±Ø­Ù…Ù„Ù‡
        peak_days = sorted(daily_stats.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            'hourly_distribution': dict(hourly_stats),
            'daily_distribution': dict(daily_stats),
            'peak_hours': peak_hours,
            'peak_days': peak_days
        }
    
    def analyze_geographic_patterns(self) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ IP Ù‡Ø§ (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Ø¯Ø§Ø±Ø¯)"""
        # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ GeoIP Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆØ¯
        geo_stats = {
            'countries': defaultdict(int),
            'suspicious_countries': []
        }
        
        # Ù„ÛŒØ³Øª Ú©Ø´ÙˆØ±Ù‡Ø§ÛŒ Ù¾Ø±Ø®Ø·Ø± (Ù…Ø«Ø§Ù„)
        high_risk_countries = ['CN', 'RU', 'KP', 'IR']
        
        # Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ØŒ Ø§Ø² IP ranges Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        for log in self.logs:
            ip = log['ip']
            # Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ GeoIP Ø¯Ø§Ø±Ø¯
            # geo_stats['countries']['Unknown'] += 1
        
        return geo_stats
    
    def analyze_bots(self) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø³Ø±ÛŒØ¹ Ø¨Ø§Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Threading Ùˆ Ø¨Ø¯ÙˆÙ† DNS"""
        import concurrent.futures
        from threading import Lock
        import time
        from collections import defaultdict, Counter

        print("    âš¡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Threading Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³Ø±ÛŒØ¹ Ø¨Ø§Øªâ€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† DNS)...")
        start_time = time.time()

        # Ø³Ø§Ø®ØªØ§Ø± Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ thread-safe locks
        result_lock = Lock()

        bot_analysis = {
            'legitimate': defaultdict(lambda: {
                'ips': set(), 
                'requests': 0, 
                'unique_urls': set(), 
                'first_seen': None, 
                'last_seen': None,
                'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡: Ø´Ù…Ø§Ø±Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù‡Ø± IP
            }),
            'potentially_legitimate': defaultdict(lambda: {
                'ips': set(), 
                'requests': 0, 
                'unique_urls': set(), 
                'first_seen': None, 
                'last_seen': None,
                'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
            }),
            'fake': defaultdict(lambda: {
                'ips': set(), 
                'requests': 0, 
                'user_agents': Counter(), 
                'patterns': Counter(), 
                'first_seen': None, 
                'last_seen': None,
                'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
            }),
            'unknown': {
                'ips': set(), 
                'requests': 0, 
                'user_agents': Counter(), 
                'unique_urls': set(), 
                'first_seen': None, 
                'last_seen': None,
                'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
            },
            'bot_activity': defaultdict(lambda: defaultdict(int)),  # Ù…Ø·Ù…Ø¦Ù† Ø´Ø¯Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯
            'bot_traffic_distribution': defaultdict(lambda: defaultdict(int)),
            'bot_ip_distribution': defaultdict(lambda: defaultdict(int))
        }

        # Progress tracking
        processed_count = 0
        progress_lock = Lock()

        def process_log_chunk(logs_chunk, chunk_id):
            """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ø¨Ø®Ø´ Ø§Ø² Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† DNS"""
            local_results = {
                'legitimate': defaultdict(lambda: {
                    'ips': set(), 
                    'requests': 0, 
                    'unique_urls': set(), 
                    'first_seen': None, 
                    'last_seen': None,
                    'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                }),
                'potentially_legitimate': defaultdict(lambda: {
                    'ips': set(), 
                    'requests': 0, 
                    'unique_urls': set(), 
                    'first_seen': None, 
                    'last_seen': None,
                    'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                }),
                'fake': defaultdict(lambda: {
                    'ips': set(), 
                    'requests': 0, 
                    'user_agents': Counter(), 
                    'patterns': Counter(), 
                    'first_seen': None, 
                    'last_seen': None,
                    'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                }),
                'unknown': {
                    'ips': set(), 
                    'requests': 0, 
                    'user_agents': Counter(), 
                    'unique_urls': set(), 
                    'first_seen': None, 
                    'last_seen': None,
                    'ip_requests': defaultdict(int)  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                },
                'bot_activity': defaultdict(lambda: defaultdict(int)),
                'bot_traffic_distribution': defaultdict(lambda: defaultdict(int))
            }

            # Ú©Ø´ Ù…Ø­Ù„ÛŒ Ø¨Ø±Ø§ÛŒ IP Ù‡Ø§
            local_ip_cache = {}

            for log in logs_chunk:
                ip = log['ip']
                ua = log['user_agent']
                url = log['url']
                dt = log['datetime']
                ua_lower = ua.lower()

                # Ú©Ù„ÛŒØ¯ Ú©Ø´
                cache_key = f"{ip}:{ua}"

                # Ú†Ú© Ú©Ø´ Ù…Ø­Ù„ÛŒ
                if cache_key in local_ip_cache:
                    bot_type, bot_category = local_ip_cache[cache_key]

                    if bot_category == 'legitimate':
                        local_results['legitimate'][bot_type]['ips'].add(ip)
                        local_results['legitimate'][bot_type]['requests'] += 1
                        local_results['legitimate'][bot_type]['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                        local_results['legitimate'][bot_type]['unique_urls'].add(url)
                        if not local_results['legitimate'][bot_type]['first_seen'] or dt < local_results['legitimate'][bot_type]['first_seen']:
                            local_results['legitimate'][bot_type]['first_seen'] = dt
                        if not local_results['legitimate'][bot_type]['last_seen'] or dt > local_results['legitimate'][bot_type]['last_seen']:
                            local_results['legitimate'][bot_type]['last_seen'] = dt
                        local_results['bot_activity'][url][bot_type] += 1
                        local_results['bot_traffic_distribution'][bot_type][dt.hour] += 1

                    elif bot_category == 'potentially_legitimate':
                        local_results['potentially_legitimate'][bot_type]['ips'].add(ip)
                        local_results['potentially_legitimate'][bot_type]['requests'] += 1
                        local_results['potentially_legitimate'][bot_type]['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                        local_results['potentially_legitimate'][bot_type]['unique_urls'].add(url)
                        local_results['bot_activity'][url][bot_type] += 1
                        local_results['bot_traffic_distribution'][bot_type][dt.hour] += 1

                    elif bot_category == 'fake':
                        local_results['fake'][bot_type]['ips'].add(ip)
                        local_results['fake'][bot_type]['requests'] += 1
                        local_results['fake'][bot_type]['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                        local_results['fake'][bot_type]['user_agents'][ua] += 1
                        local_results['bot_activity'][url][bot_type] += 1
                        local_results['bot_traffic_distribution'][bot_type][dt.hour] += 1

                    else:  # unknown
                        local_results['unknown']['ips'].add(ip)
                        local_results['unknown']['requests'] += 1
                        local_results['unknown']['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                        local_results['unknown']['unique_urls'].add(url)
                        local_results['bot_activity'][url]['Unknown'] += 1
                        local_results['bot_traffic_distribution']['Unknown'][dt.hour] += 1

                    continue
                
                # ... (Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ ØªØ´Ø®ÛŒØµ Ø¨Ø§Øª Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨ÙˆØ¯)

                # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¨Ø§Øª Ø¨Ø¯ÙˆÙ† DNS
                identified = False
                bot_type = None
                bot_category = None

                # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
                for bot_name, bot_info in self.legitimate_bots.items():
                    if not bot_info['patterns']:
                        continue

                    # Ø¨Ø±Ø±Ø³ÛŒ User-Agent
                    if any(pattern in ua_lower for pattern in bot_info['patterns']):
                        # Ø¨Ø±Ø±Ø³ÛŒ IP range Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
                        ip_matches = False
                        if bot_info['ip_ranges']:
                            try:
                                ip_obj = ipaddress.ip_address(ip)
                                # ÙÙ‚Ø· 5 range Ø§ÙˆÙ„ Ø±Ø§ Ú†Ú© Ú©Ù† Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª
                                for ip_range in bot_info['ip_ranges'][:5]:
                                    try:
                                        if '/' in ip_range:
                                            network = ipaddress.ip_network(ip_range, strict=False)
                                            if ip_obj in network:
                                                ip_matches = True
                                                break
                                    except:
                                        continue
                                    
                                if ip_matches:
                                    bot_type = bot_name
                                    bot_category = 'legitimate'
                                else:
                                    # UA Ø¯Ø±Ø³ØªØŒ IP ØºÙ„Ø·
                                    bot_type = bot_name
                                    bot_category = 'potentially_legitimate'
                            except:
                                # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ UA
                                bot_type = bot_name
                                bot_category = 'legitimate'
                        else:
                            # Ø§Ú¯Ø± IP range Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ UA
                            bot_type = bot_name
                            bot_category = 'legitimate'

                        identified = True
                        break
                    
                # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ
                if not identified:
                    # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡Ú©
                    for tool in ['nikto', 'sqlmap', 'nmap', 'burp', 'acunetix', 'wpscan', 'metasploit', 
                                'python-requests', 'curl/', 'wget/', 'libwww-perl', 'python/', 'scrapy']:
                        if tool in ua_lower:
                            bot_type = f"Hacking Tool: {tool}"
                            bot_category = 'fake'
                            identified = True
                            break
                        
                    # User-Agent Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù…Ø´Ú©ÙˆÚ©
                    if not identified:
                        if ua == '-' or len(ua) < 5:
                            bot_type = "Empty/Invalid UA"
                            bot_category = 'fake'
                            identified = True
                        elif any(word in ua_lower for word in ['bot', 'crawler', 'spider', 'scraper']):
                            # Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ© Ú©Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³ØªÙ†Ø¯
                            for legit_bot in self.legitimate_bots.keys():
                                if legit_bot.lower() in ua_lower:
                                    break
                            else:
                                bot_type = "Suspicious Bot"
                                bot_category = 'fake'
                                identified = True

                # Ø§Ú¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯ØŒ Ù†Ø§Ø´Ù†Ø§Ø³ Ø§Ø³Øª
                if not identified:
                    bot_type = 'Unknown'
                    bot_category = 'unknown'

                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ Ù…Ø­Ù„ÛŒ
                local_ip_cache[cache_key] = (bot_type, bot_category)

                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
                if bot_category == 'legitimate':
                    local_results['legitimate'][bot_type]['ips'].add(ip)
                    local_results['legitimate'][bot_type]['requests'] += 1
                    local_results['legitimate'][bot_type]['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                    local_results['legitimate'][bot_type]['unique_urls'].add(url)
                    if not local_results['legitimate'][bot_type]['first_seen'] or dt < local_results['legitimate'][bot_type]['first_seen']:
                        local_results['legitimate'][bot_type]['first_seen'] = dt
                    if not local_results['legitimate'][bot_type]['last_seen'] or dt > local_results['legitimate'][bot_type]['last_seen']:
                        local_results['legitimate'][bot_type]['last_seen'] = dt
                    local_results['bot_activity'][url][bot_type] += 1
                    local_results['bot_traffic_distribution'][bot_type][dt.hour] += 1

                elif bot_category == 'potentially_legitimate':
                    local_results['potentially_legitimate'][bot_type]['ips'].add(ip)
                    local_results['potentially_legitimate'][bot_type]['requests'] += 1
                    local_results['potentially_legitimate'][bot_type]['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                    local_results['potentially_legitimate'][bot_type]['unique_urls'].add(url)
                    if not local_results['potentially_legitimate'][bot_type]['first_seen'] or dt < local_results['potentially_legitimate'][bot_type]['first_seen']:
                        local_results['potentially_legitimate'][bot_type]['first_seen'] = dt
                    if not local_results['potentially_legitimate'][bot_type]['last_seen'] or dt > local_results['potentially_legitimate'][bot_type]['last_seen']:
                        local_results['potentially_legitimate'][bot_type]['last_seen'] = dt
                    local_results['bot_activity'][url][bot_type] += 1
                    local_results['bot_traffic_distribution'][bot_type][dt.hour] += 1

                elif bot_category == 'fake':
                    local_results['fake'][bot_type]['ips'].add(ip)
                    local_results['fake'][bot_type]['requests'] += 1
                    local_results['fake'][bot_type]['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                    local_results['fake'][bot_type]['user_agents'][ua] += 1
                    local_results['fake'][bot_type]['patterns'][ua_lower] += 1
                    if not local_results['fake'][bot_type]['first_seen'] or dt < local_results['fake'][bot_type]['first_seen']:
                        local_results['fake'][bot_type]['first_seen'] = dt
                    if not local_results['fake'][bot_type]['last_seen'] or dt > local_results['fake'][bot_type]['last_seen']:
                        local_results['fake'][bot_type]['last_seen'] = dt
                    local_results['bot_activity'][url][bot_type] += 1
                    local_results['bot_traffic_distribution'][bot_type][dt.hour] += 1

                else:  # unknown
                    local_results['unknown']['ips'].add(ip)
                    local_results['unknown']['requests'] += 1
                    local_results['unknown']['ip_requests'][ip] += 1  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
                    local_results['unknown']['user_agents'][ua] += 1
                    local_results['unknown']['unique_urls'].add(url)
                    if not local_results['unknown']['first_seen'] or dt < local_results['unknown']['first_seen']:
                        local_results['unknown']['first_seen'] = dt
                    if not local_results['unknown']['last_seen'] or dt > local_results['unknown']['last_seen']:
                        local_results['unknown']['last_seen'] = dt
                    local_results['bot_activity'][url]['Unknown'] += 1
                    local_results['bot_traffic_distribution']['Unknown'][dt.hour] += 1

            # Ø¢Ù¾Ø¯ÛŒØª progress
            nonlocal processed_count
            with progress_lock:
                processed_count += len(logs_chunk)
                progress = (processed_count / len(self.logs)) * 100
                if chunk_id % 5 == 0:
                    elapsed = time.time() - start_time
                    speed = processed_count / elapsed if elapsed > 0 else 0
                    eta = (len(self.logs) - processed_count) / speed if speed > 0 else 0
                    print(f"      Thread {chunk_id}: {processed_count:,}/{len(self.logs):,} ({progress:.1f}%) - Ø³Ø±Ø¹Øª: {speed:.0f} log/s - ETA: {eta:.0f}s")

            return local_results

        def merge_results(main_results, chunk_results):
            """Ø§Ø¯ØºØ§Ù… Ù†ØªØ§ÛŒØ¬ chunk Ø¨Ø§ Ù†ØªØ§ÛŒØ¬ Ø§ØµÙ„ÛŒ"""
            with result_lock:
                # Ø§Ø¯ØºØ§Ù… legitimate
                for bot_name, data in chunk_results['legitimate'].items():
                    main_results['legitimate'][bot_name]['ips'].update(data['ips'])
                    main_results['legitimate'][bot_name]['requests'] += data['requests']
                    main_results['legitimate'][bot_name]['unique_urls'].update(data['unique_urls'])

                    # Ø§Ø¯ØºØ§Ù… ip_requests
                    for ip, count in data['ip_requests'].items():
                        main_results['legitimate'][bot_name]['ip_requests'][ip] += count

                    if data['first_seen']:
                        if main_results['legitimate'][bot_name]['first_seen'] is None or data['first_seen'] < main_results['legitimate'][bot_name]['first_seen']:
                            main_results['legitimate'][bot_name]['first_seen'] = data['first_seen']

                    if data['last_seen']:
                        if main_results['legitimate'][bot_name]['last_seen'] is None or data['last_seen'] > main_results['legitimate'][bot_name]['last_seen']:
                            main_results['legitimate'][bot_name]['last_seen'] = data['last_seen']

                # Ø§Ø¯ØºØ§Ù… potentially_legitimate
                for bot_name, data in chunk_results['potentially_legitimate'].items():
                    main_results['potentially_legitimate'][bot_name]['ips'].update(data['ips'])
                    main_results['potentially_legitimate'][bot_name]['requests'] += data['requests']
                    main_results['potentially_legitimate'][bot_name]['unique_urls'].update(data['unique_urls'])

                    # Ø§Ø¯ØºØ§Ù… ip_requests
                    for ip, count in data['ip_requests'].items():
                        main_results['potentially_legitimate'][bot_name]['ip_requests'][ip] += count

                    if data.get('first_seen'):
                        if main_results['potentially_legitimate'][bot_name]['first_seen'] is None or data['first_seen'] < main_results['potentially_legitimate'][bot_name]['first_seen']:
                            main_results['potentially_legitimate'][bot_name]['first_seen'] = data['first_seen']

                    if data.get('last_seen'):
                        if main_results['potentially_legitimate'][bot_name]['last_seen'] is None or data['last_seen'] > main_results['potentially_legitimate'][bot_name]['last_seen']:
                            main_results['potentially_legitimate'][bot_name]['last_seen'] = data['last_seen']

                # Ø§Ø¯ØºØ§Ù… fake
                for fake_type, data in chunk_results['fake'].items():
                    main_results['fake'][fake_type]['ips'].update(data['ips'])
                    main_results['fake'][fake_type]['requests'] += data['requests']

                    # Ø§Ø¯ØºØ§Ù… ip_requests
                    for ip, count in data['ip_requests'].items():
                        main_results['fake'][fake_type]['ip_requests'][ip] += count

                    for ua, count in data['user_agents'].items():
                        main_results['fake'][fake_type]['user_agents'][ua] += count

                    for pattern, count in data['patterns'].items():
                        main_results['fake'][fake_type]['patterns'][pattern] += count

                    if data.get('first_seen'):
                        if main_results['fake'][fake_type]['first_seen'] is None or data['first_seen'] < main_results['fake'][fake_type]['first_seen']:
                            main_results['fake'][fake_type]['first_seen'] = data['first_seen']

                    if data.get('last_seen'):
                        if main_results['fake'][fake_type]['last_seen'] is None or data['last_seen'] > main_results['fake'][fake_type]['last_seen']:
                            main_results['fake'][fake_type]['last_seen'] = data['last_seen']

                # Ø§Ø¯ØºØ§Ù… unknown
                main_results['unknown']['ips'].update(chunk_results['unknown']['ips'])
                main_results['unknown']['requests'] += chunk_results['unknown']['requests']
                main_results['unknown']['unique_urls'].update(chunk_results['unknown']['unique_urls'])

                # Ø§Ø¯ØºØ§Ù… ip_requests
                for ip, count in chunk_results['unknown']['ip_requests'].items():
                    main_results['unknown']['ip_requests'][ip] += count

                for ua, count in chunk_results['unknown']['user_agents'].items():
                    main_results['unknown']['user_agents'][ua] += count

                if chunk_results['unknown'].get('first_seen'):
                    if main_results['unknown']['first_seen'] is None or chunk_results['unknown']['first_seen'] < main_results['unknown']['first_seen']:
                        main_results['unknown']['first_seen'] = chunk_results['unknown']['first_seen']

                if chunk_results['unknown'].get('last_seen'):
                    if main_results['unknown']['last_seen'] is None or chunk_results['unknown']['last_seen'] > main_results['unknown']['last_seen']:
                        main_results['unknown']['last_seen'] = chunk_results['unknown']['last_seen']

                # Ø§Ø¯ØºØ§Ù… bot_activity
                for url, bots_dict in chunk_results['bot_activity'].items():
                    for bot_type, count in bots_dict.items():
                        main_results['bot_activity'][url][bot_type] += count

                # Ø§Ø¯ØºØ§Ù… bot_traffic_distribution
                for bot_type, hours_dict in chunk_results['bot_traffic_distribution'].items():
                    for hour, count in hours_dict.items():
                        main_results['bot_traffic_distribution'][bot_type][hour] += count

        # ØªÙ‚Ø³ÛŒÙ… Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ù‡ chunk Ù‡Ø§
        total_logs = len(self.logs)
        chunk_size = 3000  # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©ÙˆÚ†Ú©ØªØ± Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
        num_workers = min(16, max(4, (total_logs // chunk_size) + 1))  # Ø¨ÛŒÙ† 4 ØªØ§ 16 thread

        chunks = [self.logs[i:i+chunk_size] for i in range(0, total_logs, chunk_size)]
        print(f"      ØªØ¹Ø¯Ø§Ø¯ chunks: {len(chunks)}, ØªØ¹Ø¯Ø§Ø¯ workers: {num_workers}")
        print(f"      Ù¾Ø±Ø¯Ø§Ø²Ø´ {total_logs:,} Ù„Ø§Ú¯...")

        # Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ Ø¨Ø§ ThreadPoolExecutor
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
            # Ø§Ø±Ø³Ø§Ù„ Ù‡Ù…Ù‡ chunk Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
            futures = []
            for i, chunk in enumerate(chunks):
                future = executor.submit(process_log_chunk, chunk, i)
                futures.append(future)

            # Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬ Ùˆ Ø§Ø¯ØºØ§Ù…
            completed = 0
            for future in concurrent.futures.as_completed(futures):
                try:
                    chunk_result = future.result()
                    merge_results(bot_analysis, chunk_result)
                    completed += 1

                    if completed % 10 == 0:
                        print(f"      Chunks completed: {completed}/{len(chunks)}")

                except Exception as e:
                    print(f"      âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ chunk: {e}")

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        print("      ğŸ“Š Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ...")

        # Ø¢Ù…Ø§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
        for bot_type in list(bot_analysis['legitimate'].keys()):
            bot_analysis['legitimate'][bot_type]['ips_count'] = len(bot_analysis['legitimate'][bot_type]['ips'])
            bot_analysis['legitimate'][bot_type]['unique_urls_count'] = len(bot_analysis['legitimate'][bot_type]['unique_urls'])

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ top URLs
            top_urls = []
            if 'bot_activity' in bot_analysis:  # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ú©Ù„ÛŒØ¯
                for url, bots_dict in bot_analysis['bot_activity'].items():
                    if bot_type in bots_dict:
                        top_urls.append((url, bots_dict[bot_type]))
            bot_analysis['legitimate'][bot_type]['top_urls'] = sorted(top_urls, key=lambda x: x[1], reverse=True)[:10]

        # Ø¢Ù…Ø§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
        for bot_type in list(bot_analysis['potentially_legitimate'].keys()):
            bot_analysis['potentially_legitimate'][bot_type]['ips_count'] = len(bot_analysis['potentially_legitimate'][bot_type]['ips'])
            bot_analysis['potentially_legitimate'][bot_type]['unique_urls_count'] = len(bot_analysis['potentially_legitimate'][bot_type]['unique_urls'])

        # Ø¢Ù…Ø§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ
        for bot_type in list(bot_analysis['fake'].keys()):
            bot_analysis['fake'][bot_type]['ips_count'] = len(bot_analysis['fake'][bot_type]['ips'])
            bot_analysis['fake'][bot_type]['top_user_agents'] = bot_analysis['fake'][bot_type]['user_agents'].most_common(5)
            bot_analysis['fake'][bot_type]['top_patterns'] = bot_analysis['fake'][bot_type]['patterns'].most_common(5)

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ top URLs
            top_urls = []
            if 'bot_activity' in bot_analysis:  # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ú©Ù„ÛŒØ¯
                for url, bots_dict in bot_analysis['bot_activity'].items():
                    if bot_type in bots_dict:
                        top_urls.append((url, bots_dict[bot_type]))
            bot_analysis['fake'][bot_type]['top_urls'] = sorted(top_urls, key=lambda x: x[1], reverse=True)[:10]

        # Ø¢Ù…Ø§Ø± Ù†Ø§Ø´Ù†Ø§Ø³
        bot_analysis['unknown']['ips_count'] = len(bot_analysis['unknown']['ips'])
        bot_analysis['unknown']['unique_urls_count'] = len(bot_analysis['unknown']['unique_urls'])
        bot_analysis['unknown']['top_user_agents'] = bot_analysis['unknown']['user_agents'].most_common(10)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ top URLs Ø¨Ø±Ø§ÛŒ unknown
        unknown_urls = []
        if 'bot_activity' in bot_analysis:  # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ú©Ù„ÛŒØ¯
            for url, bots_dict in bot_analysis['bot_activity'].items():
                if 'Unknown' in bots_dict:
                    unknown_urls.append((url, bots_dict['Unknown']))
        bot_analysis['unknown']['top_urls'] = sorted(unknown_urls, key=lambda x: x[1], reverse=True)[:10]

        elapsed_time = time.time() - start_time
        print(f"      âœ… ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Øªâ€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ø¯Ø± {elapsed_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")

        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±
        total_bot_requests = sum(b['requests'] for b in bot_analysis['legitimate'].values())
        total_potentially = sum(b['requests'] for b in bot_analysis['potentially_legitimate'].values())
        total_fake_requests = sum(b['requests'] for b in bot_analysis['fake'].values())
        total_unknown_requests = bot_analysis['unknown']['requests']

        print(f"      ğŸ“Š Ø®Ù„Ø§ØµÙ‡: Ù…Ø¹ØªØ¨Ø±: {total_bot_requests:,} | Ø§Ø­ØªÙ…Ø§Ù„ÛŒ: {total_potentially:,} | Ø¬Ø¹Ù„ÛŒ: {total_fake_requests:,} | Ù†Ø§Ø´Ù†Ø§Ø³: {total_unknown_requests:,}")

        return bot_analysis

    def comprehensive_analysis(self) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ù‡Ù…Ù‡ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ"""
        print("âš™ï¸ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø§Ù…Ù†ÛŒØªÛŒ...")
        analysis = {
            'overview': {},
            'risk_scores': {},
            'bot_analysis': {
                'legitimate': defaultdict(list),
                'fake': defaultdict(list),
                'unknown': []
            },
            'attack_analysis': defaultdict(lambda: defaultdict(list)),
            'temporal_analysis': {},
            'geographic_analysis': {},
            'suspicious_patterns': defaultdict(list),
        }

        # 1. Overview
        unique_ips = set(log['ip'] for log in self.logs)
        analysis['overview'] = {
            'total_requests': len(self.logs),
            'unique_ips': len(unique_ips),
            'date_range': {
                'start': min(log['datetime'] for log in self.logs),
                'end': max(log['datetime'] for log in self.logs)
            },
            'total_bandwidth': sum(log['bytes'] for log in self.logs),
            'error_rate': sum(1 for log in self.logs if log['status_code'] >= 400) / len(self.logs) * 100
        }

        # 2. Risk Scoring Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ IP Ù‡Ø§
        print("  ğŸ“Š Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø±ÛŒØ³Ú©...")
        for ip in unique_ips:
            risk_info = self.calculate_ip_risk_score(ip)
            analysis['risk_scores'][ip] = risk_info
            if risk_info['risk_level'] in ['CRITICAL', 'HIGH']:
                self.suspicious_ips.add(ip)
            if risk_info['risk_level'] == 'CRITICAL':
                self.critical_ips.add(ip)

        # 3. Bot Analysis (enhanced)
        print("  ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Øªâ€ŒÙ‡Ø§...")
        bot_analysis = self.analyze_bots()

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Øªâ€ŒÙ‡Ø§ Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§ØµÙ„ÛŒ
        analysis['bot_analysis'] = {
            'legitimate': bot_analysis['legitimate'],
            'potentially_legitimate': bot_analysis['potentially_legitimate'],
            'fake': bot_analysis['fake'],
            'unknown': bot_analysis['unknown'],
            'traffic_distribution': bot_analysis['bot_traffic_distribution'],
            'activity': bot_analysis['bot_activity']
        }

        # 4. Attack Pattern Analysis
        print("  ğŸ¯ ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø­Ù…Ù„Ù‡...")
        for log in self.logs:
            request = log['request'] + ' ' + log['url']
            ip = log['ip']
            for attack_type, attack_info in self.advanced_attack_patterns.items():
                for pattern in attack_info['patterns']:
                    if re.search(pattern, request, re.IGNORECASE):
                        analysis['attack_analysis'][attack_type][ip].append({
                            'timestamp': log['timestamp'],
                            'request': log['request'][:200],
                            'status_code': log['status_code'],
                            'pattern': pattern[:50]
                        })
                        if attack_info['severity'] in ['CRITICAL', 'HIGH']:
                            self.suspicious_ips.add(ip)
                        break
                    
        # 5. Temporal Analysis
        print("  â° ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù†ÛŒ...")
        analysis['temporal_analysis'] = self.analyze_temporal_patterns()

        return analysis
    
    def generate_recommendations(self, analysis: Dict) -> List[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ù†ÛŒØªÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„"""
        recommendations = []
        
        # Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ IP Ù‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ©
        suspicious_count = len(self.suspicious_ips)
        if suspicious_count > 100:
            recommendations.append("ğŸ”´ ÙˆØ¶Ø¹ÛŒØª Ø¨Ø­Ø±Ø§Ù†ÛŒ: Ø¨ÛŒØ´ Ø§Ø² 100 IP Ù…Ø´Ú©ÙˆÚ©. ÙÙˆØ±Ø§Ù‹ ÙØ§ÛŒØ±ÙˆØ§Ù„ Ø±Ø§ ØªÙ‚ÙˆÛŒØª Ú©Ù†ÛŒØ¯.")
        elif suspicious_count > 50:
            recommendations.append("ğŸŸ  ÙˆØ¶Ø¹ÛŒØª Ù‡Ø´Ø¯Ø§Ø±: IP Ù‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ© Ø²ÛŒØ§Ø¯. Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ù† Ú©Ø±Ø¯Ù† Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.")
        
        # Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø­Ù…Ù„Ø§Øª
        if 'sql_injection' in analysis['attack_analysis']:
            recommendations.append("ğŸ’‰ Ø­Ù…Ù„Ø§Øª SQL Injection Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ú©Ù†ÛŒØ¯.")
        
        if 'xss' in analysis['attack_analysis']:
            recommendations.append("ğŸ“ Ø­Ù…Ù„Ø§Øª XSS Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ encode Ú©Ù†ÛŒØ¯.")
        
        if 'lfi_rfi' in analysis['attack_analysis']:
            recommendations.append("ğŸ“ Ø­Ù…Ù„Ø§Øª File Inclusion Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. Ø¯Ø³ØªØ±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†ÛŒØ¯.")
        
        if 'command_injection' in analysis['attack_analysis']:
            recommendations.append("ğŸ’» Ø­Ù…Ù„Ø§Øª Command Injection Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…ÛŒ Ø±Ø§ ÙÛŒÙ„ØªØ± Ú©Ù†ÛŒØ¯.")
        
        # Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ
        if analysis['bot_analysis']['fake']:
            recommendations.append(f"ğŸ¤– {len(analysis['bot_analysis']['fake'])} Ø¨Ø§Øª Ø¬Ø¹Ù„ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯. User-Agent verification ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯.")
        
        # Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø³Ø§ÛŒØª
        if self.site_type == 'wordpress':
            if any('xmlrpc' in str(attack) for attack in analysis['attack_analysis'].values()):
                recommendations.append("ğŸ”’ xmlrpc.php Ø±Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯.")
            recommendations.append("ğŸ›¡ï¸ Ù¾Ù„Ø§Ú¯ÛŒÙ† Ø§Ù…Ù†ÛŒØªÛŒ Wordfence ÛŒØ§ Sucuri Ù†ØµØ¨ Ú©Ù†ÛŒØ¯.")
            recommendations.append("ğŸ”„ Ù‡Ù…Ù‡ Ù¾Ù„Ø§Ú¯ÛŒÙ†â€ŒÙ‡Ø§ Ùˆ ØªÙ…â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯.")
        
        elif self.site_type == 'opencart':
            recommendations.append("ğŸ” Ù…Ø³ÛŒØ± Ù¾Ù†Ù„ Ø§Ø¯Ù…ÛŒÙ† Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯.")
            recommendations.append("ğŸ“ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ /system/ Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†ÛŒØ¯.")
            recommendations.append("ğŸ”„ Ù‡Ù…Ù‡ Ø§ÙØ²ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯.")
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ
        recommendations.append("â˜ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CDN/WAF Ù…Ø«Ù„ Cloudflare Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯.")
        recommendations.append("ğŸ“Š Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ real-time Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.")
        recommendations.append("ğŸ” 2FA Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¯Ù…ÛŒÙ† ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯.")
        recommendations.append("ğŸ’¾ Ø¨Ú©Ø§Ù¾ Ù…Ù†Ø¸Ù… Ø§Ø² Ø³Ø§ÛŒØª ØªÙ‡ÛŒÙ‡ Ú©Ù†ÛŒØ¯.")
        
        return recommendations
    
    def export_to_excel(self, analysis: Dict, filename: str = 'security_report.xlsx'):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Excel Ø¬Ø§Ù…Ø¹"""
        print(f"\nğŸ“Š ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Excel...")
        
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            
            # 1. Overview Sheet
            overview_df = pd.DataFrame([analysis['overview']])
            overview_df.to_excel(writer, sheet_name='Overview', index=False)
            
            # 2. Risk Scores Sheet
            risk_data = []
            for ip, risk_info in analysis['risk_scores'].items():
                risk_data.append({
                    'IP': ip,
                    'Risk Score': risk_info['score'],
                    'Risk Level': risk_info['risk_level'],
                    'Request Count': risk_info['request_count'],
                    'Unique URLs': risk_info['unique_urls'],
                    'Error Count': risk_info['error_count'],
                    'Reasons': ' | '.join(risk_info['reasons'][:3])
                })
            
            risk_df = pd.DataFrame(risk_data)
            risk_df = risk_df.sort_values('Risk Score', ascending=False)
            risk_df.to_excel(writer, sheet_name='Risk Analysis', index=False)
            
            # 3. Critical IPs Sheet
            critical_data = []
            for ip in self.critical_ips:
                ip_logs = [log for log in self.logs if log['ip'] == ip]
                critical_data.append({
                    'IP': ip,
                    'Total Requests': len(ip_logs),
                    'First Seen': min(log['datetime'] for log in ip_logs),
                    'Last Seen': max(log['datetime'] for log in ip_logs),
                    'Risk Score': analysis['risk_scores'][ip]['score'],
                    'Main Reason': analysis['risk_scores'][ip]['reasons'][0] if analysis['risk_scores'][ip]['reasons'] else 'N/A'
                })
            
            if critical_data:
                critical_df = pd.DataFrame(critical_data)
                critical_df.to_excel(writer, sheet_name='Critical IPs', index=False)
            
            # 4. Attack Patterns Sheet
            attack_data = []
            for attack_type, ip_dict in analysis['attack_analysis'].items():
                for ip, attacks in ip_dict.items():
                    attack_data.append({
                        'Attack Type': attack_type.replace('_', ' ').title(),
                        'IP': ip,
                        'Count': len(attacks),
                        'Severity': self.advanced_attack_patterns[attack_type]['severity'],
                        'First Attack': attacks[0]['timestamp'] if attacks else 'N/A',
                        'Sample Request': attacks[0]['request'][:100] if attacks else 'N/A'
                    })
            
            if attack_data:
                attack_df = pd.DataFrame(attack_data)
                attack_df = attack_df.sort_values(['Severity', 'Count'], ascending=[True, False])
                attack_df.to_excel(writer, sheet_name='Attack Patterns', index=False)
            
            # 5. Bot Analysis Sheet - Ø¨Ø§ Ø´Ù…Ø§Ø±Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø± Ø­Ø³Ø¨ IP
            bot_data = []

            # Legitimate bots
            for company, bot_info in analysis['bot_analysis']['legitimate'].items():
                # Ø¨Ø±Ø§ÛŒ Ù‡Ø± IP Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
                for ip in list(bot_info.get('ips', set()))[:100]:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ† IP Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø§Øª
                    ip_request_count = 0
                    for log in self.logs:
                        if log['ip'] == ip:
                            ua_lower = log['user_agent'].lower()
                            if any(pattern in ua_lower for pattern in self.legitimate_bots.get(company, {}).get('patterns', [])):
                                ip_request_count += 1

                    bot_data.append({
                        'Type': 'Legitimate',
                        'Company': company,
                        'IP': ip,
                        'Verification': 'VERIFIED',
                        'Bot Type': company,
                        'Requests': ip_request_count  # ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ† IP
                    })

            # Potentially legitimate bots
            if 'potentially_legitimate' in analysis['bot_analysis']:
                for company, bot_info in analysis['bot_analysis']['potentially_legitimate'].items():
                    for ip in list(bot_info.get('ips', set()))[:100]:
                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ† IP
                        ip_request_count = 0
                        for log in self.logs:
                            if log['ip'] == ip:
                                ua_lower = log['user_agent'].lower()
                                if any(pattern in ua_lower for pattern in self.legitimate_bots.get(company, {}).get('patterns', [])):
                                    ip_request_count += 1

                        bot_data.append({
                            'Type': 'Potentially Legitimate',
                            'Company': company,
                            'IP': ip,
                            'Verification': 'PARTIAL',
                            'Bot Type': company,
                            'Requests': ip_request_count
                        })

            # Fake bots
            for bot_type, bot_info in analysis['bot_analysis']['fake'].items():
                for ip in list(bot_info.get('ips', set()))[:100]:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ† IP
                    ip_request_count = sum(1 for log in self.logs if log['ip'] == ip)

                    bot_data.append({
                        'Type': 'FAKE',
                        'Company': bot_type,
                        'IP': ip,
                        'Verification': 'FAILED',
                        'Bot Type': 'Suspicious',
                        'Requests': ip_request_count
                    })

            # Unknown bots
            if 'unknown' in analysis['bot_analysis']:
                unknown_info = analysis['bot_analysis']['unknown']
                for ip in list(unknown_info.get('ips', set()))[:100]:
                    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§ÛŒÙ† IP
                    ip_request_count = sum(1 for log in self.logs if log['ip'] == ip)

                    bot_data.append({
                        'Type': 'Unknown',
                        'Company': 'Unknown',
                        'IP': ip,
                        'Verification': 'N/A',
                        'Bot Type': 'Unknown',
                        'Requests': ip_request_count
                    })

            if bot_data:
                bot_df = pd.DataFrame(bot_data)
                # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Type Ùˆ ØªØ¹Ø¯Ø§Ø¯ Requests
                bot_df = bot_df.sort_values(['Type', 'Requests'], ascending=[True, False])
                bot_df.to_excel(writer, sheet_name='Bot Analysis', index=False)
                        
            # 6. Temporal Analysis Sheet
            temporal_data = []
            for hour, count in analysis['temporal_analysis']['hourly_distribution'].items():
                temporal_data.append({
                    'Hour': f"{hour:02d}:00",
                    'Request Count': count
                })
            
            if temporal_data:
                temporal_df = pd.DataFrame(temporal_data)
                temporal_df = temporal_df.sort_values('Hour')
                temporal_df.to_excel(writer, sheet_name='Temporal Analysis', index=False)
            
            # 7. Top Requests Sheet
            request_counter = Counter()
            for log in self.logs:
                request_counter[log['url']] += 1
            
            top_requests = []
            for url, count in request_counter.most_common(100):
                top_requests.append({
                    'URL': url[:150],
                    'Count': count,
                    'Percentage': f"{(count/len(self.logs)*100):.2f}%"
                })
            
            top_requests_df = pd.DataFrame(top_requests)
            top_requests_df.to_excel(writer, sheet_name='Top Requests', index=False)
            
            # 8. Bot Error Analysis Sheet
            bot_error_data = []
            for log in self.logs:
                if log['status_code'] >= 400:
                    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† User-Agent Ø¨Ø§Øª Ø§Ø³Øª
                    ua_lower = log['user_agent'].lower()
                    is_bot = False
                    bot_type = None 

                    # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
                    for company, bot_info in self.legitimate_bots.items():
                        if any(pattern in ua_lower for pattern in bot_info['patterns']):
                            is_bot = True
                            bot_type = f"Legitimate {company}"
                            break   

                    # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ
                    if not is_bot:
                        if any(tool in ua_lower for tool in self.suspicious_user_agents['hacking_tools']):
                            is_bot = True
                            bot_type = "Hacking Tool"
                        elif any(bot in ua_lower for bot in ['bot', 'crawler', 'spider']):
                            is_bot = True
                            bot_type = "Suspicious Bot" 

                    if is_bot:
                        bot_error_data.append({
                            'IP': log['ip'],
                            'Status Code': log['status_code'],
                            'Error Type': self.error_codes.get(str(log['status_code']), 'Unknown'),
                            'URL': log['url'][:100],
                            'Timestamp': log['timestamp'],
                            'User Agent': log['user_agent'][:150],
                            'Bot Type': bot_type
                        })  

            if bot_error_data[:1000]:  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 1000 Ø±Ú©ÙˆØ±Ø¯
                bot_error_df = pd.DataFrame(bot_error_data[:1000])
                bot_error_df.to_excel(writer, sheet_name='Bot Error Analysis', index=False)
            
            # 9. User Agent Analysis Sheet
            ua_counter = Counter(log['user_agent'] for log in self.logs)
            ua_data = []
            
            for ua, count in ua_counter.most_common(50):
                ua_type = 'Unknown'
                if ua == '-' or len(ua) < 5:
                    ua_type = 'Empty/Invalid'
                elif any(tool in ua.lower() for tool in self.suspicious_user_agents['hacking_tools']):
                    ua_type = 'Hacking Tool'
                elif any(bot in ua.lower() for bot in ['bot', 'crawler', 'spider']):
                    ua_type = 'Bot/Crawler'
                else:
                    try:
                        parsed = parse(ua)
                        if parsed.is_mobile:
                            ua_type = 'Mobile'
                        elif parsed.is_tablet:
                            ua_type = 'Tablet'
                        elif parsed.is_pc:
                            ua_type = 'Desktop'
                    except:
                        pass
                    
                ua_data.append({
                    'User Agent': ua[:150],
                    'Count': count,
                    'Type': ua_type,
                    'Percentage': f"{(count/len(self.logs)*100):.2f}%"
                })
            
            ua_df = pd.DataFrame(ua_data)
            ua_df.to_excel(writer, sheet_name='User Agents', index=False)
            
            # 10. Bot Statistics Sheet - Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡
            bot_stats_data = []
            
            # Ø¢Ù…Ø§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
            for company, bot_info in analysis['bot_analysis']['legitimate'].items():
                if bot_info.get('requests', 0) > 0:
                    bot_stats_data.append({
                        'Bot Name': company,
                        'Type': 'Legitimate',
                        'Total Requests': bot_info.get('requests', 0),
                        'Unique IPs': bot_info.get('ips_count', len(bot_info.get('ips', []))),
                        'Unique URLs': bot_info.get('unique_urls_count', len(bot_info.get('unique_urls', []))),
                        'First Seen': bot_info.get('first_seen', 'N/A'),
                        'Last Seen': bot_info.get('last_seen', 'N/A')
                    })
            
            # Ø¢Ù…Ø§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
            if 'potentially_legitimate' in analysis['bot_analysis']:
                for company, bot_info in analysis['bot_analysis']['potentially_legitimate'].items():
                    if bot_info.get('requests', 0) > 0:
                        bot_stats_data.append({
                            'Bot Name': company,
                            'Type': 'Potentially Legitimate',
                            'Total Requests': bot_info.get('requests', 0),
                            'Unique IPs': bot_info.get('ips_count', len(bot_info.get('ips', []))),
                            'Unique URLs': bot_info.get('unique_urls_count', len(bot_info.get('unique_urls', []))),
                            'First Seen': bot_info.get('first_seen', 'N/A'),
                            'Last Seen': bot_info.get('last_seen', 'N/A')
                        })
            
            # Ø¢Ù…Ø§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ
            for bot_type, bot_info in analysis['bot_analysis']['fake'].items():
                if bot_info.get('requests', 0) > 0:
                    bot_stats_data.append({
                        'Bot Name': bot_type,
                        'Type': 'Fake/Suspicious',
                        'Total Requests': bot_info.get('requests', 0),
                        'Unique IPs': bot_info.get('ips_count', len(bot_info.get('ips', []))),
                        'Unique URLs': 'N/A',
                        'First Seen': bot_info.get('first_seen', 'N/A'),
                        'Last Seen': bot_info.get('last_seen', 'N/A')
                    })
            
            if bot_stats_data:
                bot_stats_df = pd.DataFrame(bot_stats_data)
                bot_stats_df = bot_stats_df.sort_values(['Type', 'Total Requests'], ascending=[True, False])
                bot_stats_df.to_excel(writer, sheet_name='Bot Statistics', index=False)
            
            # Formatting
            workbook = writer.book
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ù‡ Ù‡Ù…Ù‡ sheet Ù‡Ø§
            for sheet_name in workbook.sheetnames:
                worksheet = workbook[sheet_name]
                
                # Header styling
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_alignment = Alignment(horizontal="center", vertical="center")
                
                for cell in worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = header_alignment
                
                # Auto-adjust column widths
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if cell.value:
                                max_length = max(max_length, len(str(cell.value)))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
                
                # Conditional formatting for Risk Analysis sheet
                if sheet_name == 'Risk Analysis':
                    for row in range(2, len(risk_df) + 2):
                        risk_level = worksheet[f'C{row}'].value
                        if risk_level == 'CRITICAL':
                            fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
                        elif risk_level == 'HIGH':
                            fill = PatternFill(start_color="FFA500", end_color="FFA500", fill_type="solid")
                        elif risk_level == 'MEDIUM':
                            fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
                        elif risk_level == 'LOW':
                            fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")
                        else:
                            fill = PatternFill(start_color="00FF00", end_color="00FF00", fill_type="solid")
                        
                        worksheet[f'C{row}'].fill = fill
                
                # Conditional formatting for Bot Analysis sheet
                if sheet_name == 'Bot Analysis':
                    for row in range(2, worksheet.max_row + 1):
                        bot_type = worksheet[f'A{row}'].value
                        if bot_type == 'FAKE':
                            fill = PatternFill(start_color="FF6B6B", end_color="FF6B6B", fill_type="solid")
                            for col in range(1, 7):  # ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
                                worksheet.cell(row=row, column=col).fill = fill
                        elif bot_type == 'Potentially Legitimate':
                            fill = PatternFill(start_color="FFE66D", end_color="FFE66D", fill_type="solid")
                            for col in range(1, 7):
                                worksheet.cell(row=row, column=col).fill = fill
                        elif bot_type == 'Legitimate':
                            fill = PatternFill(start_color="A8E6CF", end_color="A8E6CF", fill_type="solid")
                            for col in range(1, 7):
                                worksheet.cell(row=row, column=col).fill = fill
        
        print(f"âœ… Ú¯Ø²Ø§Ø±Ø´ Excel Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return filename
    
    def export_firewall_rules(self):
        """ØªÙˆÙ„ÛŒØ¯ Ù‚ÙˆØ§Ù†ÛŒÙ† ÙØ§ÛŒØ±ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        
        # 2. .htaccess for Apache
        with open('htaccess_rules.txt', 'w') as f:
            f.write(f"# Security Rules - Generated: {datetime.now()}\n")
            f.write(f"# Add to your .htaccess file\n\n")
            
            f.write("# Block suspicious IPs\n")
            f.write("order allow,deny\n")
            
            for ip in sorted(self.suspicious_ips):
                f.write(f"deny from {ip}\n")
            
            f.write("allow from all\n\n")
            
            # Additional security headers
            f.write("# Security Headers\n")
            f.write("Header set X-Frame-Options \"SAMEORIGIN\"\n")
            f.write("Header set X-Content-Type-Options \"nosniff\"\n")
            f.write("Header set X-XSS-Protection \"1; mode=block\"\n")
        
        # 3. nginx configuration
        with open('nginx_rules.conf', 'w') as f:
            f.write(f"# Nginx Security Rules - Generated: {datetime.now()}\n\n")
            
            f.write("# Block suspicious IPs\n")
            for ip in sorted(self.suspicious_ips):
                f.write(f"deny {ip};\n")
            
            f.write("\n# Rate limiting\n")
            f.write("limit_req_zone $binary_remote_addr zone=one:10m rate=10r/s;\n")
            f.write("limit_conn_zone $binary_remote_addr zone=addr:10m;\n")
        
        # 4. CSF (ConfigServer Firewall)
        with open('csf_deny.txt', 'w') as f:
            f.write(f"# CSF Deny List - Generated: {datetime.now()}\n")
            f.write("# Copy to: /etc/csf/csf.deny\n\n")
            
            for ip in sorted(self.suspicious_ips):
                risk_score = self.analysis_results.get('risk_scores', {}).get(ip, {}).get('score', 0)
                f.write(f"{ip} # Risk Score: {risk_score}\n")
        
        # 5. fail2ban configuration
        with open('fail2ban_jail.conf', 'w') as f:
            f.write(f"# Fail2ban Jail Configuration - Generated: {datetime.now()}\n\n")
            
            f.write("[web-security]\n")
            f.write("enabled = true\n")
            f.write("filter = web-security\n")
            f.write("action = iptables-multiport[name=WebSecurity, port=\"http,https\"]\n")
            f.write("logpath = /var/log/apache2/access.log\n")
            f.write("maxretry = 3\n")
            f.write("findtime = 600\n")
            f.write("bantime = 86400\n")
        
        print("âœ… Ù‚ÙˆØ§Ù†ÛŒÙ† ÙØ§ÛŒØ±ÙˆØ§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯:")
        print("  â€¢ iptables_rules.sh")
        print("  â€¢ htaccess_rules.txt")
        print("  â€¢ nginx_rules.conf")
        print("  â€¢ csf_deny.txt")
        print("  â€¢ fail2ban_jail.conf")
    
    def generate_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ"""
        print("" + "="*80)
        print(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ø§Ù…Ù†ÛŒØªÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡")
        print(f"ğŸ• Ø²Ù…Ø§Ù†: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“Œ Ù†ÙˆØ¹ Ø³Ø§ÛŒØª: {self.site_type.upper()}")
        print("="*80)

        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹
        self.analysis_results = self.comprehensive_analysis()

        # 1. Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª
        print("#### ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ø§Ù…Ù†ÛŒØªÛŒ")
        print("-" * 60)
        overview = self.analysis_results['overview']
        print(f"  â€¢ Ú©Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {overview['total_requests']:,}")
        print(f"  â€¢ IP Ù‡Ø§ÛŒ ÛŒÚ©ØªØ§: {overview['unique_ips']:,}")
        print(f"  â€¢ IP Ù‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ©: {len(self.suspicious_ips)} ({len(self.suspicious_ips)/overview['unique_ips']*100:.1f}%)")
        print(f"  â€¢ IP Ù‡Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {len(self.critical_ips)}")
        print(f"  â€¢ Ø­Ø¬Ù… ØªØ±Ø§ÙÛŒÚ©: {overview['total_bandwidth']/1024/1024:.2f} MB")
        print(f"  â€¢ Ù†Ø±Ø® Ø®Ø·Ø§: {overview['error_rate']:.2f}%")

        # 2. ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ø¨Ø­Ø±Ø§Ù†ÛŒ
        critical_threats = [ip for ip, info in self.analysis_results['risk_scores'].items() 
                          if info['risk_level'] == 'CRITICAL']
        if critical_threats:
            print("#### ğŸš¨ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ø¨Ø­Ø±Ø§Ù†ÛŒ (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ù‚Ø¯Ø§Ù… ÙÙˆØ±ÛŒ)")
            print("-" * 60)
            for ip in critical_threats[:10]:
                risk_info = self.analysis_results['risk_scores'][ip]
                print(f"  ğŸ”´ {ip}")
                print(f"     Ø§Ù…ØªÛŒØ§Ø² Ø±ÛŒØ³Ú©: {risk_info['score']}")
                print(f"     Ø¯Ù„Ø§ÛŒÙ„: {', '.join(risk_info['reasons'][:3])}")

        # 3. ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Øªâ€ŒÙ‡Ø§ Ùˆ Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¢Ù†Ù‡Ø§
        print("#### ğŸ¤– ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Øªâ€ŒÙ‡Ø§ Ùˆ Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø¯ÛŒØ¯")
        print("-" * 60)

        # Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
        legitimate_bots = self.analysis_results['bot_analysis']['legitimate']
        if legitimate_bots:
            print("âœ… Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±:")
            for bot_type, data in legitimate_bots.items():
                if data['requests'] > 0:
                    print(f"**{bot_type}:**")
                    print(f"      â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {data['requests']:,}")
                    print(f"      â€¢ IP Ù‡Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯: {data['ips_count']}")
                    print(f"      â€¢ Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø²Ø¯ÛŒØ¯: {data['first_seen'].strftime('%Y-%m-%d %H:%M:%S') if data['first_seen'] else 'N/A'}")
                    print(f"      â€¢ Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø§Ø²Ø¯ÛŒØ¯: {data['last_seen'].strftime('%Y-%m-%d %H:%M:%S') if data['last_seen'] else 'N/A'}")
                    print(f"      â€¢ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯: {', '.join([url for url, _ in data['top_urls'][:3]])}")

        # Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù…Ø¹ØªØ¨Ø±
        potentially_legit = self.analysis_results['bot_analysis']['potentially_legitimate']
        if potentially_legit:
            print("ğŸŸ¡ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù…Ø¹ØªØ¨Ø± (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ):")
            for bot_type, data in potentially_legit.items():
                if data['requests'] > 0:
                    print(f"**{bot_type}:**")
                    print(f"      â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {data['requests']:,}")
                    print(f"      â€¢ IP Ù‡Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯: {data['ips_count']}")
                    print(f"      â€¢ Ø¯Ù„ÛŒÙ„: User-Agent Ù…Ø¹ØªØ¨Ø± Ø§Ù…Ø§ IP range ØªØ£ÛŒÛŒØ¯ Ù†Ø´Ø¯Ù‡")

        # Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ
        fake_bots = self.analysis_results['bot_analysis']['fake']
        if fake_bots:
            print("âŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡:")
            for bot_type, data in fake_bots.items():
                if data['requests'] > 0:
                    print(f"**{bot_type}:**")
                    print(f"      â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {data['requests']:,}")
                    print(f"      â€¢ IP Ù‡Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯: {data['ips_count']}")
                    print(f"      â€¢ User-AgentÙ‡Ø§ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„: {', '.join([ua for ua, _ in data['top_user_agents'][:3]])}")
                    print(f"      â€¢ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯: {', '.join([url for url, _ in data['top_urls'][:3]])}")

        # Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø³
        unknown_bots = self.analysis_results['bot_analysis']['unknown']
        if unknown_bots['requests'] > 0:
            print("âšª Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø³:")
            print(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {unknown_bots['requests']:,}")
            print(f"  â€¢ IP Ù‡Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯: {unknown_bots['ips_count']}")
            print(f"  â€¢ User-AgentÙ‡Ø§ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„: {', '.join([ua for ua, _ in unknown_bots['top_user_agents'][:3]])}")
            print(f"  â€¢ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯: {', '.join([url for url, _ in unknown_bots['top_urls'][:3]])}")

        # 4. Ø­Ù…Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
        if self.analysis_results['attack_analysis']:
            print("#### ğŸ¯ Ø­Ù…Ù„Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡")
            print("-" * 60)
            attack_summary = Counter()
            for attack_type, ip_dict in self.analysis_results['attack_analysis'].items():
                attack_summary[attack_type] = len(ip_dict)
            for attack_type, count in attack_summary.most_common():
                severity = self.advanced_attack_patterns[attack_type]['severity']
                icon = {'CRITICAL': 'ğŸ”´', 'HIGH': 'ğŸŸ ', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢'}.get(severity, 'âšª')
                print(f"  {icon} {attack_type.replace('_', ' ').title()}: {count} IP")

        # 5. Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        temporal = self.analysis_results['temporal_analysis']
        if temporal.get('peak_hours'):
            print("#### â° Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ")
            print("-" * 60)
            print("  Ø³Ø§Ø¹Ø§Øª Ù¾Ø±ØªØ±Ø§ÙÛŒÚ©:")
            for hour, count in temporal['peak_hours'][:5]:
                print(f"    â€¢ {hour:02d}:00 - {count:,} Ø¯Ø±Ø®ÙˆØ§Ø³Øª")


        # 7. Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ
        print("#### ğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ")
        print("-" * 60)
        # ØªÙˆØ²ÛŒØ¹ Ø³Ø·Ø­ Ø±ÛŒØ³Ú©
        risk_distribution = Counter(info['risk_level'] for info in self.analysis_results['risk_scores'].values())
        print("  ØªÙˆØ²ÛŒØ¹ Ø³Ø·Ø­ Ø±ÛŒØ³Ú© IP Ù‡Ø§:")
        for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'SAFE']:
            count = risk_distribution.get(level, 0)
            if count > 0:
                percentage = count / overview['unique_ips'] * 100
                bar = 'â–ˆ' * int(percentage / 2)
                print(f"    {level:8s}: {count:4d} ({percentage:5.1f}%) {bar}")

        return self.analysis_results
    
    def analyze_bot_visit_times(self) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        bot_visits = {
            'legitimate_bot_times': defaultdict(lambda: {
                'visits': [],
                'total_visits': 0,
                'first_visit': None,
                'last_visit': None,
                'hourly_distribution': defaultdict(int),
                'daily_distribution': defaultdict(int),
                'weekly_distribution': defaultdict(int),
                'peak_hours': [],
                'off_hours_visits': 0,  # Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¯Ø± Ø³Ø§Ø¹Ø§Øª ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ
                'unique_ips': set(),
                'unique_urls': set(),
                'average_requests_per_ip': 0,
                'crawl_rate': 0,  # ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡
                'response_codes': defaultdict(int)
            }),
            'fake_bot_times': defaultdict(lambda: {
                'visits': [],
                'claimed': '',
                'reason': '',
                'user_agents': set(),
                'ips': set()
            }),
            'potentially_legitimate': defaultdict(lambda: {
                'visits': [],
                'total_visits': 0,
                'unique_ips': set(),
                'verification_status': 'partial'
            }),
            'bot_behavior_analysis': {
                'crawl_patterns': defaultdict(list),
                'suspicious_patterns': [],
                'bot_comparison': {}
            }
        }
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ dnspython
        DNSPYTHON_AVAILABLE = False
        dns_resolver = None
        try:
            import dns.resolver
            import dns.reversename
            DNSPYTHON_AVAILABLE = True
            dns_resolver = dns.resolver.Resolver()
            dns_resolver.timeout = 0.3
            dns_resolver.lifetime = 1.0
            dns_resolver.nameservers = ['8.8.8.8', '8.8.4.4']
        except ImportError:
            DNSPYTHON_AVAILABLE = False
        
        # Ú©Ø´ DNS Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ performance
        dns_cache = {}
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡ Ù„Ø§Ú¯â€ŒÙ‡Ø§
        for log in self.logs:
            ip = log['ip']
            ua = log['user_agent']
            url = log['url']
            dt = log['datetime']
            status_code = log['status_code']
            ua_lower = ua.lower()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øª Ú¯ÙˆÚ¯Ù„ Ø¨Ø§ GoogleBotVerifier
            google_result = self.google_verifier.verify_google_bot(ip, ua)
            if google_result['is_google']:
                bot_type = 'Google'
                bot_visits['legitimate_bot_times'][bot_type]['visits'].append(dt)
                bot_visits['legitimate_bot_times'][bot_type]['hourly_distribution'][dt.hour] += 1
                bot_visits['legitimate_bot_times'][bot_type]['daily_distribution'][dt.weekday()] += 1
                bot_visits['legitimate_bot_times'][bot_type]['unique_ips'].add(ip)
                bot_visits['legitimate_bot_times'][bot_type]['unique_urls'].add(url)
                bot_visits['legitimate_bot_times'][bot_type]['response_codes'][status_code] += 1
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¯Ø± Ø³Ø§Ø¹Ø§Øª ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ (Ø¨ÛŒÙ† 2 ØªØ§ 6 ØµØ¨Ø­)
                if 2 <= dt.hour <= 6:
                    bot_visits['legitimate_bot_times'][bot_type]['off_hours_visits'] += 1
                continue
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØ± Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
            bot_identified = False
            for company, bot_info in self.legitimate_bots.items():
                if company == 'Google':  # Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡
                    continue
                    
                if not bot_info['patterns']:
                    continue
                
                # Ø¨Ø±Ø±Ø³ÛŒ User-Agent
                ua_matches = any(pattern in ua_lower for pattern in bot_info['patterns'])
                
                if not ua_matches:
                    continue
                
                # Ø¨Ø±Ø±Ø³ÛŒ IP range
                ip_matches = False
                if bot_info['ip_ranges']:
                    try:
                        ip_obj = ipaddress.ip_address(ip)
                        for ip_range in bot_info['ip_ranges']:
                            if '/' in ip_range:
                                network = ipaddress.ip_network(ip_range, strict=False)
                                if ip_obj in network:
                                    ip_matches = True
                                    break
                            else:
                                if ip.startswith(ip_range.split('/')[0]):
                                    ip_matches = True
                                    break
                    except Exception:
                        pass
                    
                # Ø§Ú¯Ø± IP range ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø±Ø¯
                if bot_info['ip_ranges'] and ip_matches:
                    # Ø¨Ø§Øª Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø¹ØªØ¨Ø±
                    bot_visits['legitimate_bot_times'][company]['visits'].append(dt)
                    bot_visits['legitimate_bot_times'][company]['hourly_distribution'][dt.hour] += 1
                    bot_visits['legitimate_bot_times'][company]['daily_distribution'][dt.weekday()] += 1
                    bot_visits['legitimate_bot_times'][company]['unique_ips'].add(ip)
                    bot_visits['legitimate_bot_times'][company]['unique_urls'].add(url)
                    bot_visits['legitimate_bot_times'][company]['response_codes'][status_code] += 1
                    
                    if 2 <= dt.hour <= 6:
                        bot_visits['legitimate_bot_times'][company]['off_hours_visits'] += 1
                    
                    bot_identified = True
                    break
                    
                # Ø§Ú¯Ø± IP range ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ø§ DNS Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…
                elif not bot_info['ip_ranges'] and bot_info['dns_suffix']:
                    # DNS verification
                    hostname = None
                    if ip in dns_cache:
                        hostname = dns_cache[ip]
                    #else:
                    #    if DNSPYTHON_AVAILABLE and dns_resolver:
                    #        try:
                    #            rev_name = dns.reversename.from_address(ip)
                    #            hostname = str(dns_resolver.resolve(rev_name, "PTR")[0])
                    #            dns_cache[ip] = hostname
                    #        except:
                    #            dns_cache[ip] = None
                    #    else:
                    #        try:
                    #            hostname = socket.gethostbyaddr(ip)[0]
                    #            dns_cache[ip] = hostname
                    #        except:
                    #            dns_cache[ip] = None
                    
                    if hostname:
                        dns_matches = any(hostname.endswith(suffix) for suffix in bot_info['dns_suffix'])
                        if dns_matches:
                            # Ø¨Ø§Øª Ù…Ø¹ØªØ¨Ø± Ø¨Ø§ DNS verification
                            bot_visits['legitimate_bot_times'][company]['visits'].append(dt)
                            bot_visits['legitimate_bot_times'][company]['hourly_distribution'][dt.hour] += 1
                            bot_visits['legitimate_bot_times'][company]['daily_distribution'][dt.weekday()] += 1
                            bot_visits['legitimate_bot_times'][company]['unique_ips'].add(ip)
                            bot_visits['legitimate_bot_times'][company]['unique_urls'].add(url)
                            bot_visits['legitimate_bot_times'][company]['response_codes'][status_code] += 1
                            
                            if 2 <= dt.hour <= 6:
                                bot_visits['legitimate_bot_times'][company]['off_hours_visits'] += 1
                            
                            bot_identified = True
                            break
                        else:
                            # User-Agent Ø¯Ø±Ø³Øª Ø§Ù…Ø§ DNS ØªØ£ÛŒÛŒØ¯ Ù†Ø´Ø¯ - Ù…Ø´Ú©ÙˆÚ©
                            bot_visits['fake_bot_times'][ip]['visits'].append(dt)
                            bot_visits['fake_bot_times'][ip]['claimed'] = company
                            bot_visits['fake_bot_times'][ip]['reason'] = 'DNS verification failed'
                            bot_visits['fake_bot_times'][ip]['user_agents'].add(ua)
                            bot_visits['fake_bot_times'][ip]['ips'].add(ip)
                            bot_identified = True
                            break
                        
                # Ø§Ú¯Ø± IP range ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ø§Ù…Ø§ Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø¯Ø§Ø±Ø¯
                elif bot_info['ip_ranges'] and not ip_matches:
                    # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ø§Øª Ø¬Ø¹Ù„ÛŒ ÛŒØ§ potentially legitimate
                    bot_visits['potentially_legitimate'][company]['visits'].append(dt)
                    bot_visits['potentially_legitimate'][company]['total_visits'] += 1
                    bot_visits['potentially_legitimate'][company]['unique_ips'].add(ip)
                    bot_visits['potentially_legitimate'][company]['verification_status'] = 'UA matches, IP doesn\'t match'
                    bot_identified = True
                    break
                
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ© Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡Ú©
            if not bot_identified:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù‡Ú©
                for tool in self.suspicious_user_agents['hacking_tools']:
                    if tool in ua_lower:
                        bot_visits['fake_bot_times'][ip]['visits'].append(dt)
                        bot_visits['fake_bot_times'][ip]['claimed'] = 'Hacking Tool'
                        bot_visits['fake_bot_times'][ip]['reason'] = f'Contains: {tool}'
                        bot_visits['fake_bot_times'][ip]['user_agents'].add(ua)
                        bot_visits['fake_bot_times'][ip]['ips'].add(ip)
                        bot_identified = True
                        break
                    
                # Ø¨Ø±Ø±Ø³ÛŒ User-Agent Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù…Ø´Ú©ÙˆÚ©
                if not bot_identified and (ua == '-' or len(ua) < 5):
                    bot_visits['fake_bot_times'][ip]['visits'].append(dt)
                    bot_visits['fake_bot_times'][ip]['claimed'] = 'Invalid UA'
                    bot_visits['fake_bot_times'][ip]['reason'] = 'Empty or too short User-Agent'
                    bot_visits['fake_bot_times'][ip]['user_agents'].add(ua)
                    bot_visits['fake_bot_times'][ip]['ips'].add(ip)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
        for company, data in bot_visits['legitimate_bot_times'].items():
            if data['visits']:
                data['total_visits'] = len(data['visits'])
                data['first_visit'] = min(data['visits'])
                data['last_visit'] = max(data['visits'])
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø¹Ø§Øª Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯
                peak_hours = sorted(data['hourly_distribution'].items(), 
                                  key=lambda x: x[1], reverse=True)
                data['peak_hours'] = [hour for hour, _ in peak_hours[:5]]
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± IP
                if data['unique_ips']:
                    data['average_requests_per_ip'] = data['total_visits'] / len(data['unique_ips'])
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® crawl (Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø± Ø¯Ù‚ÛŒÙ‚Ù‡)
                if data['first_visit'] and data['last_visit']:
                    time_diff = (data['last_visit'] - data['first_visit']).total_seconds() / 60
                    if time_diff > 0:
                        data['crawl_rate'] = data['total_visits'] / time_diff
        
        # ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ø¨Ø§Øªâ€ŒÙ‡Ø§
        bot_visits['bot_behavior_analysis'] = self._analyze_bot_behavior(bot_visits)
        
        return bot_visits
    
    def _analyze_bot_behavior(self, bot_visits: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ùˆ Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¨Ø§Øªâ€ŒÙ‡Ø§"""
        behavior_analysis = {
            'crawl_patterns': {},
            'suspicious_patterns': [],
            'bot_comparison': {},
        }
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒ crawl Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø§Øª Ù…Ø¹ØªØ¨Ø±
        for company, data in bot_visits['legitimate_bot_times'].items():
            if data['total_visits'] > 0:
                pattern = {
                    'regularity': 'regular' if data['crawl_rate'] < 10 else 'aggressive',
                    'peak_activity': f"{data['peak_hours'][0]:02d}:00" if data['peak_hours'] else 'N/A',
                    'off_hours_percentage': (data['off_hours_visits'] / data['total_visits']) * 100,
                    'error_rate': (data['response_codes'].get(404, 0) / data['total_visits']) * 100,
                    'urls_per_ip': len(data['unique_urls']) / len(data['unique_ips']) if data['unique_ips'] else 0
                }
                behavior_analysis['crawl_patterns'][company] = pattern
                
                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ©
                if pattern['regularity'] == 'aggressive':
                    behavior_analysis['suspicious_patterns'].append(
                        f"{company}: Aggressive crawling rate ({data['crawl_rate']:.1f} req/min)"
                    )
                
                if pattern['error_rate'] > 20:
                    behavior_analysis['suspicious_patterns'].append(
                        f"{company}: High error rate ({pattern['error_rate']:.1f}%)"
                    )
        
        # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§Øªâ€ŒÙ‡Ø§
        if bot_visits['legitimate_bot_times']:
            total_bot_visits = sum(data['total_visits'] 
                                  for data in bot_visits['legitimate_bot_times'].values())
            for company, data in bot_visits['legitimate_bot_times'].items():
                if data['total_visits'] > 0:
                    behavior_analysis['bot_comparison'][company] = {
                        'percentage_of_bot_traffic': (data['total_visits'] / total_bot_visits) * 100,
                        'unique_ips': len(data['unique_ips']),
                        'unique_urls': len(data['unique_urls'])
                    }
        
        return behavior_analysis
    
    def export_json_report(self, filename: str = 'security_report.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ù‡ ÙØ±Ù…Øª JSON Ø¨Ø§ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ encoding"""
        report = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'site_type': self.site_type,
                'log_file': self.log_file_path,
                'analysis_version': '2.0'
            },
            'statistics': {
                'total_requests': len(self.logs),
                'unique_ips': len(set(log['ip'] for log in self.logs)),
                'suspicious_ips': len(self.suspicious_ips),
                'critical_ips': len(self.critical_ips),
                'fake_bots': len(self.analysis_results['bot_analysis']['fake'])
            },
            'threats': {
                'critical_ips': list(self.critical_ips),
                'suspicious_ips': list(self.suspicious_ips),
                'fake_bots': dict(self.analysis_results['bot_analysis']['fake'])
            },
            'bot_visit_times': self.analyze_bot_visit_times(),
            'risk_scores': {
                ip: {
                    'score': info['score'],
                    'level': info['risk_level'],
                    'reasons': info['reasons']
                }
                for ip, info in sorted(
                    self.analysis_results['risk_scores'].items(),
                    key=lambda x: x[1]['score'],
                    reverse=True
                )[:100]  # Top 100 risky IPs
            },
            'attack_patterns': {
                attack_type: {
                    'severity': self.advanced_attack_patterns[attack_type]['severity'],
                    'affected_ips': list(ip_dict.keys())[:20]
                }
                for attack_type, ip_dict in self.analysis_results['attack_analysis'].items()
            },
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø§ encoding UTF-8
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nâœ… Ú¯Ø²Ø§Ø±Ø´ JSON Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        return filename

    def generate_bot_timeline_report(self) -> Dict:
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø®Ø· Ø²Ù…Ø§Ù†ÛŒ (Timeline) Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¨Ø§Øªâ€ŒÙ‡Ø§"""
        from collections import defaultdict
        from datetime import datetime, timedelta
        import json

        print("\nâ° ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø®Ø· Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§...")

        timeline = {
            'search_engines': defaultdict(list),  # Google, Bing, etc.
            'ai_bots': defaultdict(list),  # OpenAI, Perplexity, etc.
            'social_media': defaultdict(list),  # Facebook, LinkedIn, etc.
            'seo_tools': defaultdict(list),  # Ahrefs, SemRush, etc.
            'other_bots': defaultdict(list),
            'timeline_events': [],  # Ù‡Ù…Ù‡ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª chronological
            'hourly_summary': defaultdict(lambda: defaultdict(int)),
            'daily_summary': defaultdict(lambda: defaultdict(int)),
            'bot_page_analysis': defaultdict(lambda: defaultdict(list))
        }

        # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§
        bot_categories = {
            'search_engines': ['Google', 'Bing', 'Yandex', 'Baidu', 'DuckDuckGo'],
            'ai_bots': ['OpenAI', 'PerplexityBot', 'PerplexityUser', 'Cohere', 'Mistral', 
                        'AllenInstitute', 'YouCom'],
            'social_media': ['Meta', 'LinkedIn', 'ByteDance'],
            'seo_tools': ['SemRush', 'Ahrefs'],
            'other_bots': ['Amazon', 'Apple', 'CommonCrawl', 'Diffbot', 'Omgili', 'Timpi']
        }

        # Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±
        processed_entries = set()

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§
        for log in sorted(self.logs, key=lambda x: x['datetime']):
            ip = log['ip']
            ua = log['user_agent']
            url = log['url']
            dt = log['datetime']
            status_code = log['status_code']
            ua_lower = ua.lower()

            # Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±
            entry_key = f"{dt}_{ip}_{url}"
            if entry_key in processed_entries:
                continue
            processed_entries.add(entry_key)

            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¨Ø§Øª
            bot_identified = False
            bot_name = None
            bot_category = None
            bot_subtype = None

            # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§Øª Ú¯ÙˆÚ¯Ù„ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±
            google_result = self.google_verifier.verify_google_bot(ip, ua)
            if google_result['is_google']:
                bot_name = 'Google'
                bot_category = 'search_engines'

                # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø§Øª Ú¯ÙˆÚ¯Ù„
                if 'googlebot' in ua_lower:
                    if 'googlebot-image' in ua_lower:
                        bot_subtype = 'Googlebot-Image'
                    elif 'googlebot-video' in ua_lower:
                        bot_subtype = 'Googlebot-Video'
                    elif 'googlebot-news' in ua_lower:
                        bot_subtype = 'Googlebot-News'
                    elif 'smartphone' in ua_lower:
                        bot_subtype = 'Googlebot-Mobile'
                    else:
                        bot_subtype = 'Googlebot'
                elif 'adsbot-google' in ua_lower:
                    bot_subtype = 'AdsBot-Google'
                elif 'mediapartners-google' in ua_lower:
                    bot_subtype = 'Mediapartners-Google'
                elif 'google-inspectiontool' in ua_lower:
                    bot_subtype = 'Google-InspectionTool'
                elif 'google-extended' in ua_lower:
                    bot_subtype = 'Google-Extended'
                elif 'google-cloudvertexbot' in ua_lower:
                    bot_subtype = 'Google-CloudVertexBot'
                elif 'googleother' in ua_lower:
                    bot_subtype = 'GoogleOther'
                else:
                    bot_subtype = 'Google-Bot'

                bot_identified = True

            # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØ± Ø¨Ø§Øªâ€ŒÙ‡Ø§
            if not bot_identified:
                for company, bot_info in self.legitimate_bots.items():
                    if company == 'Google':
                        continue
                    
                    if not bot_info['patterns']:
                        continue
                    
                    if any(pattern in ua_lower for pattern in bot_info['patterns']):
                        bot_name = company

                        # ØªØ¹ÛŒÛŒÙ† Ø¯Ø³ØªÙ‡
                        for cat, bot_list in bot_categories.items():
                            if company in bot_list:
                                bot_category = cat
                                break
                            
                        if not bot_category:
                            bot_category = 'other_bots'

                        # ØªØ¹ÛŒÛŒÙ† Ø²ÛŒØ±Ù†ÙˆØ¹ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
                        if company == 'OpenAI':
                            if 'gptbot' in ua_lower:
                                bot_subtype = 'GPTBot'
                            elif 'chatgpt-user' in ua_lower:
                                bot_subtype = 'ChatGPT-User'
                            elif 'oai-searchbot' in ua_lower:
                                bot_subtype = 'OAI-SearchBot'
                            else:
                                bot_subtype = 'OpenAI-Bot'
                        elif company == 'PerplexityBot':
                            bot_subtype = 'PerplexityBot'
                        elif company == 'PerplexityUser':
                            bot_subtype = 'Perplexity-User'
                        else:
                            bot_subtype = company

                        bot_identified = True
                        break
                    
            if bot_identified and bot_name and bot_category:
                # Ø§ÛŒØ¬Ø§Ø¯ Ø±ÙˆÛŒØ¯Ø§Ø¯ timeline
                event = {
                    'timestamp': dt.isoformat(),
                    'datetime_obj': dt,
                    'bot_name': bot_name,
                    'bot_type': bot_subtype or bot_name,
                    'category': bot_category,
                    'ip': ip,
                    'url': url,
                    'status_code': status_code,
                    'user_agent': ua[:100]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„
                }

                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ timeline Ø§ØµÙ„ÛŒ
                timeline['timeline_events'].append(event)

                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¯Ø³ØªÙ‡ Ù…Ø±Ø¨ÙˆØ·Ù‡
                timeline[bot_category][bot_name].append(event)

                # Ø¢Ù…Ø§Ø± Ø³Ø§Ø¹ØªÛŒ Ùˆ Ø±ÙˆØ²Ø§Ù†Ù‡
                hour_key = dt.strftime('%Y-%m-%d %H:00')
                day_key = dt.strftime('%Y-%m-%d')

                timeline['hourly_summary'][hour_key][bot_subtype or bot_name] += 1
                timeline['daily_summary'][day_key][bot_subtype or bot_name] += 1

                # ØªØ­Ù„ÛŒÙ„ ØµÙØ­Ø§Øª Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù‡Ø± Ø¨Ø§Øª
                timeline['bot_page_analysis'][bot_subtype or bot_name][url].append({
                    'timestamp': dt.isoformat(),
                    'ip': ip,
                    'status': status_code
                })

        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ HTML Ø²ÛŒØ¨Ø§
        self._generate_timeline_html(timeline)

        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Text Ø²ÛŒØ¨Ø§
        self._generate_timeline_text(timeline)

        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ JSON
        self._generate_timeline_json(timeline)

        return timeline 

    def _generate_timeline_html(self, timeline: Dict):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ HTML Ø§Ø² Timeline"""
        
        html_content = """
        <!DOCTYPE html>
    <html dir="rtl" lang="fa">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Bot Timeline Analysis Report</title>
        <style>
            * { margin: 0; padding: 0; box-sizing: border-box; }
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: #333;
                padding: 20px;
                direction: ltr;
            }
            .container {
                max-width: 1400px;
                margin: 0 auto;
                background: white;
                border-radius: 15px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                padding: 30px;
            }
            h1 {
                text-align: center;
                color: #764ba2;
                margin-bottom: 30px;
                font-size: 2.5em;
            }
            h2 {
                color: #667eea;
                margin: 30px 0 20px;
                border-bottom: 3px solid #667eea;
                padding-bottom: 10px;
            }
            h3 {
                color: #555;
                margin: 20px 0 10px;
            }
            .summary-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                gap: 20px;
                margin: 30px 0;
            }
            .summary-card {
                background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
                padding: 20px;
                border-radius: 10px;
                box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            }
            .summary-card h3 {
                color: #667eea;
                margin-bottom: 15px;
            }
            .timeline {
                position: relative;
                padding: 20px 0;
            }
            .timeline-item {
                display: flex;
                margin-bottom: 20px;
                position: relative;
                padding-left: 40px;
            }
            .timeline-item::before {
                content: '';
                position: absolute;
                left: 10px;
                top: 5px;
                width: 12px;
                height: 12px;
                border-radius: 50%;
                background: #667eea;
            }
            .timeline-item::after {
                content: '';
                position: absolute;
                left: 15px;
                top: 17px;
                bottom: -20px;
                width: 2px;
                background: #ddd;
            }
            .timeline-item:last-child::after {
                display: none;
            }
            .timeline-time {
                min-width: 150px;
                color: #888;
                font-weight: bold;
            }
            .timeline-content {
                flex: 1;
                background: #f8f9fa;
                padding: 15px;
                border-radius: 8px;
                margin-left: 20px;
                box-shadow: 0 3px 10px rgba(0,0,0,0.1);
            }
            .bot-google { background: linear-gradient(135deg, #4285F4, #34A853); }
            .bot-bing { background: linear-gradient(135deg, #00BCF2, #0078D4); }
            .bot-openai { background: linear-gradient(135deg, #10A37F, #1A7F64); }
            .bot-perplexity { background: linear-gradient(135deg, #8B5CF6, #7C3AED); }
            .bot-meta { background: linear-gradient(135deg, #1877F2, #42B3F4); }
            .bot-badge {
                display: inline-block;
                padding: 3px 10px;
                border-radius: 15px;
                color: white;
                font-size: 0.85em;
                font-weight: bold;
                margin-right: 10px;
            }
            .url-path {
                color: #666;
                font-family: monospace;
                background: #e9ecef;
                padding: 2px 6px;
                border-radius: 4px;
                margin: 5px 0;
            }
            .status-200 { color: #28a745; }
            .status-404 { color: #dc3545; }
            .status-301, .status-302 { color: #ffc107; }
            .chart-container {
                margin: 30px 0;
                padding: 20px;
                background: #f8f9fa;
                border-radius: 10px;
            }
            .hour-bar {
                display: flex;
                align-items: center;
                margin: 5px 0;
            }
            .hour-label {
                min-width: 60px;
                font-weight: bold;
            }
            .hour-value {
                flex: 1;
                background: linear-gradient(90deg, #667eea, #764ba2);
                padding: 5px 10px;
                border-radius: 5px;
                color: white;
                font-size: 0.9em;
            }
            table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
            }
            th {
                background: linear-gradient(135deg, #667eea, #764ba2);
                color: white;
                padding: 12px;
                text-align: left;
            }
            td {
                padding: 10px;
                border-bottom: 1px solid #ddd;
            }
            tr:hover {
                background: #f5f7fa;
            }
            .top-pages {
                list-style: none;
                padding: 0;
            }
            .top-pages li {
                background: #f8f9fa;
                margin: 10px 0;
                padding: 10px;
                border-radius: 5px;
                border-left: 4px solid #667eea;
            }
            .timestamp {
                color: #888;
                font-size: 0.85em;
            }
            @media (max-width: 768px) {
                .timeline-item {
                    flex-direction: column;
                }
                .timeline-content {
                    margin-left: 0;
                    margin-top: 10px;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ¤– Bot Timeline Analysis Report</h1>
            <p style="text-align: center; color: #888; margin-bottom: 30px;">
                Generated: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """
            </p>
    """

        # Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
        html_content += """
            <h2>ğŸ“Š Summary Statistics</h2>
            <div class="summary-grid">
        """

        # Ø´Ù…Ø§Ø±Ø´ Ø¨Ø§Øªâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙ‡
        categories_stats = {
            'Search Engines': len(timeline['search_engines']),
            'AI Bots': len(timeline['ai_bots']),
            'Social Media': len(timeline['social_media']),
            'SEO Tools': len(timeline['seo_tools']),
            'Other Bots': len(timeline['other_bots'])
        }

        for category, count in categories_stats.items():
            total_requests = sum(len(events) for events in timeline[category.lower().replace(' ', '_')].values())
            html_content += f"""
                <div class="summary-card">
                    <h3>{category}</h3>
                    <p><strong>{count}</strong> unique bots</p>
                    <p><strong>{total_requests}</strong> total requests</p>
                </div>
            """

        html_content += "</div>"

        # Timeline Ø§ØµÙ„ÛŒ
        html_content += """
            <h2>â° Complete Timeline</h2>
            <div class="timeline">
        """

        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ø®Ø±ÛŒÙ† 100 Ø±ÙˆÛŒØ¯Ø§Ø¯
        for event in timeline['timeline_events'][-100:]:
            bot_class = ""
            if 'google' in event['bot_name'].lower():
                bot_class = "bot-google"
            elif 'bing' in event['bot_name'].lower():
                bot_class = "bot-bing"
            elif 'openai' in event['bot_name'].lower():
                bot_class = "bot-openai"
            elif 'perplexity' in event['bot_name'].lower():
                bot_class = "bot-perplexity"
            elif 'meta' in event['bot_name'].lower() or 'facebook' in event['bot_name'].lower():
                bot_class = "bot-meta"

            status_class = f"status-{event['status_code']}"

            html_content += f"""
                <div class="timeline-item">
                    <div class="timeline-time">{event['datetime_obj'].strftime('%H:%M:%S')}</div>
                    <div class="timeline-content">
                        <span class="bot-badge {bot_class}">{event['bot_type']}</span>
                        <span class="timestamp">{event['datetime_obj'].strftime('%Y-%m-%d')}</span>
                        <div class="url-path">{event['url'][:100]}</div>
                        <div>
                            <small>IP: {event['ip']} | Status: <span class="{status_class}">{event['status_code']}</span></small>
                        </div>
                    </div>
                </div>
            """

        html_content += "</div>"

        # Ø¨Ø®Ø´ ØªØ­Ù„ÛŒÙ„ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ AI
        html_content += """
            <h2>ğŸ¤– AI Bots Activity</h2>
            <table>
                <thead>
                    <tr>
                        <th>Bot Name</th>
                        <th>Total Requests</th>
                        <th>First Visit</th>
                        <th>Last Visit</th>
                        <th>Top Pages</th>
                    </tr>
                </thead>
                <tbody>
        """

        for bot_name, events in timeline['ai_bots'].items():
            if events:
                first_visit = min(e['datetime_obj'] for e in events)
                last_visit = max(e['datetime_obj'] for e in events)

                # Top pages
                page_counts = {}
                for e in events:
                    page_counts[e['url']] = page_counts.get(e['url'], 0) + 1
                top_pages = sorted(page_counts.items(), key=lambda x: x[1], reverse=True)[:3]

                html_content += f"""
                    <tr>
                        <td><strong>{bot_name}</strong></td>
                        <td>{len(events)}</td>
                        <td>{first_visit.strftime('%Y-%m-%d %H:%M')}</td>
                        <td>{last_visit.strftime('%Y-%m-%d %H:%M')}</td>
                        <td>{'<br>'.join([f"{p[0][:50]} ({p[1]}x)" for p in top_pages])}</td>
                    </tr>
                """

        html_content += """
                </tbody>
            </table>
        """

        # Ø¨Ø®Ø´ ØªØ­Ù„ÛŒÙ„ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
        html_content += """
            <h2>ğŸ” Search Engines Activity</h2>
            <table>
                <thead>
                    <tr>
                        <th>Search Engine</th>
                        <th>Bot Types</th>
                        <th>Total Requests</th>
                        <th>Date Range</th>
                        <th>Crawl Rate</th>
                    </tr>
                </thead>
                <tbody>
        """

        for bot_name, events in timeline['search_engines'].items():
            if events:
                # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù Ø¨Ø§Øª
                bot_types = list(set(e['bot_type'] for e in events))
                first_visit = min(e['datetime_obj'] for e in events)
                last_visit = max(e['datetime_obj'] for e in events)

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® crawl
                time_diff = (last_visit - first_visit).total_seconds() / 60  # Ø¨Ù‡ Ø¯Ù‚ÛŒÙ‚Ù‡
                crawl_rate = len(events) / time_diff if time_diff > 0 else 0

                html_content += f"""
                    <tr>
                        <td><strong>{bot_name}</strong></td>
                        <td>{', '.join(bot_types[:3])}</td>
                        <td>{len(events)}</td>
                        <td>{first_visit.strftime('%Y-%m-%d')} to {last_visit.strftime('%Y-%m-%d')}</td>
                        <td>{crawl_rate:.1f} req/min</td>
                    </tr>
                """

        html_content += """
                </tbody>
            </table>
        """

        # Ù†Ù…ÙˆØ¯Ø§Ø± ÙØ¹Ø§Ù„ÛŒØª Ø³Ø§Ø¹ØªÛŒ
        html_content += """
            <h2>ğŸ“ˆ Hourly Activity Pattern</h2>
            <div class="chart-container">
        """

        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¢Ù…Ø§Ø± Ø³Ø§Ø¹ØªÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù…Ø±ÙˆØ²
        today = datetime.now().strftime('%Y-%m-%d')
        for hour in range(24):
            hour_key = f"{today} {hour:02d}:00"
            if hour_key in timeline['hourly_summary']:
                total = sum(timeline['hourly_summary'][hour_key].values())
                if total > 0:
                    html_content += f"""
                        <div class="hour-bar">
                            <div class="hour-label">{hour:02d}:00</div>
                            <div class="hour-value" style="width: {min(total*5, 100)}%">
                                {total} requests
                            </div>
                        </div>
                    """

        html_content += """
            </div>
        </div>
    </body>
    </html>
        """

        # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ HTML
        with open('bot_timeline_report.html', 'w', encoding='utf-8') as f:
            f.write(html_content)

        print("âœ… Ú¯Ø²Ø§Ø±Ø´ HTML Timeline Ø¯Ø± bot_timeline_report.html Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")    

    def _generate_timeline_text(self, timeline: Dict):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Text Ø§Ø² Timeline"""
        with open('bot_timeline_report.txt', 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("                    BOT TIMELINE ANALYSIS REPORT\n")
            f.write(f"                    Generated: {datetime.now()}\n")
            f.write("="*80 + "\n\n")

            # Ø¨Ø®Ø´ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
            f.write("ğŸ” SEARCH ENGINES TIMELINE\n")
            f.write("-"*60 + "\n\n")

            for bot_name, events in timeline['search_engines'].items():
                if events:
                    f.write(f"### {bot_name}\n")
                    f.write(f"Total Visits: {len(events)}\n")

                    # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø¨Ø§Øª
                    bot_types = {}
                    for e in events:
                        bot_type = e['bot_type']
                        if bot_type not in bot_types:
                            bot_types[bot_type] = []
                        bot_types[bot_type].append(e)

                    for bot_type, type_events in bot_types.items():
                        f.write(f"\n  [{bot_type}] - {len(type_events)} requests\n")

                        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ø®Ø±ÛŒÙ† 5 Ø¨Ø§Ø²Ø¯ÛŒØ¯
                        for event in type_events[-5:]:
                            f.write(f"    â€¢ {event['datetime_obj'].strftime('%Y-%m-%d %H:%M:%S')}")
                            f.write(f" | {event['url'][:60]}")
                            f.write(f" | Status: {event['status_code']}")
                            f.write(f" | IP: {event['ip']}\n")

                    f.write("\n")

            # Ø¨Ø®Ø´ Ø¨Ø§Øªâ€ŒÙ‡Ø§ÛŒ AI
            f.write("\nğŸ¤– AI BOTS TIMELINE\n")
            f.write("-"*60 + "\n\n")

            for bot_name, events in timeline['ai_bots'].items():
                if events:
                    f.write(f"### {bot_name}\n")
                    f.write(f"Total Visits: {len(events)}\n")

                    first_visit = min(e['datetime_obj'] for e in events)
                    last_visit = max(e['datetime_obj'] for e in events)

                    f.write(f"First Visit: {first_visit.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"Last Visit: {last_visit.strftime('%Y-%m-%d %H:%M:%S')}\n")

                    # ØªØ­Ù„ÛŒÙ„ ØµÙØ­Ø§Øª
                    page_counts = {}
                    for e in events:
                        page_counts[e['url']] = page_counts.get(e['url'], 0) + 1

                    top_pages = sorted(page_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                    f.write("\nTop Crawled Pages:\n")
                    for page, count in top_pages:
                        f.write(f"  â€¢ {page[:80]} ({count}x)\n")

                    # Ù†Ù…Ø§ÛŒØ´ timeline
                    f.write("\nRecent Activity:\n")
                    for event in events[-10:]:
                        f.write(f"  {event['datetime_obj'].strftime('%H:%M:%S')} - {event['url'][:60]}\n")

                    f.write("\n")

            # Ø®Ù„Ø§ØµÙ‡ Ø±ÙˆØ²Ø§Ù†Ù‡
            f.write("\nğŸ“… DAILY SUMMARY\n")
            f.write("-"*60 + "\n\n")

            for day, bots in sorted(timeline['daily_summary'].items())[-7:]:
                f.write(f"{day}:\n")
                for bot, count in sorted(bots.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"  â€¢ {bot}: {count} requests\n")
                f.write("\n")

            # Ù¾Ø±ØªØ±Ø§ÙÛŒÚ©â€ŒØªØ±ÛŒÙ† ØµÙØ­Ø§Øª
            f.write("\nğŸ“„ MOST CRAWLED PAGES BY BOTS\n")
            f.write("-"*60 + "\n\n")

            all_pages = {}
            for bot, pages in timeline['bot_page_analysis'].items():
                for page, visits in pages.items():
                    if page not in all_pages:
                        all_pages[page] = {'total': 0, 'bots': set()}
                    all_pages[page]['total'] += len(visits)
                    all_pages[page]['bots'].add(bot)

            top_pages_sorted = sorted(all_pages.items(), 
                                      key=lambda x: x[1]['total'], 
                                      reverse=True)[:20]

            for page, data in top_pages_sorted:
                f.write(f"{page[:80]}\n")
                f.write(f"  Total: {data['total']} | Bots: {', '.join(list(data['bots'])[:5])}\n\n")

        print("âœ… Ú¯Ø²Ø§Ø±Ø´ Text Timeline Ø¯Ø± bot_timeline_report.txt Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯") 

    def _generate_timeline_json(self, timeline: Dict):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ JSON Ø§Ø² Timeline"""
        # ØªØ¨Ø¯ÛŒÙ„ datetime objects Ø¨Ù‡ string Ø¨Ø±Ø§ÛŒ JSON
        json_timeline = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_events': len(timeline['timeline_events'])
            },
            'search_engines': {},
            'ai_bots': {},
            'social_media': {},
            'seo_tools': {},
            'other_bots': {},
            'hourly_summary': dict(timeline['hourly_summary']),
            'daily_summary': dict(timeline['daily_summary'])
        }

        # ØªØ¨Ø¯ÛŒÙ„ events
        for category in ['search_engines', 'ai_bots', 'social_media', 'seo_tools', 'other_bots']:
            for bot_name, events in timeline[category].items():
                if events:
                    json_timeline[category][bot_name] = {
                        'total_requests': len(events),
                        'first_visit': min(e['datetime_obj'] for e in events).isoformat(),
                        'last_visit': max(e['datetime_obj'] for e in events).isoformat(),
                        'recent_activity': [
                            {
                                'timestamp': e['timestamp'],
                                'url': e['url'],
                                'status': e['status_code'],
                                'ip': e['ip']
                            }
                            for e in events[-20:]  # Ø¢Ø®Ø±ÛŒÙ† 20 Ø±ÙˆÛŒØ¯Ø§Ø¯
                        ]
                    }

        with open('bot_timeline.json', 'w', encoding='utf-8') as f:
            json.dump(json_timeline, f, indent=2, ensure_ascii=False)

        print("âœ… Ú¯Ø²Ø§Ø±Ø´ JSON Timeline Ø¯Ø± bot_timeline.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")   
    
    def export_all_reports(self):
        """ØªÙˆÙ„ÛŒØ¯ Ù‡Ù…Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ Ø¨Ø§ Ø±ÙØ¹ Ù…Ø´Ú©Ù„ encoding"""
        print("\nğŸ“ ØªÙˆÙ„ÛŒØ¯ Ù‡Ù…Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§...")
        
        # 1. Excel Report
        excel_file = self.export_to_excel(self.analysis_results)
        
        # 2. JSON Report
        json_file = self.export_json_report()
        
        # 3. Firewall Rules
        self.export_firewall_rules()

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú¯Ø²Ø§Ø±Ø´ Timeline
        timeline_report = self.generate_bot_timeline_report()

        # 4. Ban List - Ø¨Ø§ encoding UTF-8
        with open('ban_list.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Suspicious IPs - Generated: {datetime.now()}\n")
            f.write(f"# Total: {len(self.suspicious_ips)} IPs\n\n")
            
            # Sort by risk score
            sorted_ips = sorted(
                self.suspicious_ips,
                key=lambda ip: self.analysis_results['risk_scores'].get(ip, {}).get('score', 0),
                reverse=True
            )
            
            for ip in sorted_ips:
                risk_info = self.analysis_results['risk_scores'].get(ip, {})
                reasons_str = ', '.join(risk_info.get('reasons', ['Unknown'])[:2])
                f.write(f"{ip} # Score: {risk_info.get('score', 0)}, Level: {risk_info.get('risk_level', 'Unknown')}\n")
        
        # 5. Critical IPs for immediate action - Ø¨Ø§ encoding UTF-8
        with open('critical_ips.txt', 'w', encoding='utf-8') as f:
            f.write(f"# CRITICAL IPs requiring immediate action\n")
            f.write(f"# Generated: {datetime.now()}\n\n")
            
            for ip in self.critical_ips:
                risk_info = self.analysis_results['risk_scores'].get(ip, {})
                reasons_str = ', '.join(risk_info.get('reasons', ['Unknown'])[:2])
                f.write(f"{ip} # {reasons_str}\n")
        
        # 6. Bot Visit Report - Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø¯ÛŒØ¯ Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¨Ø§Øªâ€ŒÙ‡Ø§
        bot_visits = self.analyze_bot_visit_times()
        with open('bot_visits_report.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Bot Visit Times Analysis\n")
            f.write(f"# Generated: {datetime.now()}\n\n")
            
            f.write("## LEGITIMATE BOTS\n")
            f.write("-" * 50 + "\n\n")
            
            for company, data in bot_visits['legitimate_bot_times'].items():
                if data['total_visits'] > 0:
                    f.write(f"### {company}\n")
                    f.write(f"Total Visits: {data['total_visits']}\n")
                    f.write(f"First Visit: {data['first_visit']}\n")
                    f.write(f"Last Visit: {data['last_visit']}\n")
                    f.write(f"Peak Hours: {', '.join([f'{h:02d}:00' for h in data['peak_hours']])}\n")
                    f.write(f"Unique IPs: {len(data['unique_ips'])}\n")
                    f.write(f"IP List: {', '.join(list(data['unique_ips'])[:10])}\n")
                    f.write("\n")
            
            if self.analysis_results['bot_analysis']['fake']:
                f.write("\n## FAKE BOTS DETECTED\n")
                f.write("-" * 50 + "\n\n")
                for ip, info in self.analysis_results['bot_analysis']['fake'].items():
                    f.write(f"IP: {ip}\n")
                    f.write(f"Claimed to be: {info.get('claimed', 'Unknown')}\n")
                    f.write(f"Reason: {info.get('reason', 'Failed verification')}\n\n")
        
        # 7. Summary Report (Markdown) - Ø¨Ø§ encoding UTF-8
        with open('security_summary.md', 'w', encoding='utf-8') as f:
            f.write(f"# Security Analysis Report\n")
            f.write(f"**Generated:** {datetime.now()}\n")
            f.write(f"**Site Type:** {self.site_type.upper()}\n\n")
            
            f.write("## ğŸ“Š Overview\n")
            overview = self.analysis_results['overview']
            f.write(f"- Total Requests: {overview['total_requests']:,}\n")
            f.write(f"- Unique IPs: {overview['unique_ips']:,}\n")
            f.write(f"- Suspicious IPs: {len(self.suspicious_ips)}\n")
            f.write(f"- Critical Threats: {len(self.critical_ips)}\n")
            f.write(f"- Error Rate: {overview['error_rate']:.2f}%\n\n")
            
            f.write("## ğŸ¤– Bot Analysis\n")
            bot_visits = self.analyze_bot_visit_times()
            for company, data in bot_visits['legitimate_bot_times'].items():
                if data['total_visits'] > 0:
                    f.write(f"### {company}\n")
                    f.write(f"- Visits: {data['total_visits']}\n")
                    f.write(f"- Time Range: {data['first_visit']} to {data['last_visit']}\n")
                    f.write(f"- Peak Hours: {', '.join([f'{h:02d}:00' for h in data['peak_hours'][:3]])}\n\n")
            
            f.write("## ğŸš¨ Critical Threats\n")
            for ip in list(self.critical_ips)[:10]:
                risk_info = self.analysis_results['risk_scores'][ip]
                f.write(f"### {ip}\n")
                f.write(f"- Risk Score: {risk_info['score']}\n")
                f.write(f"- Reasons: {', '.join(risk_info['reasons'][:3])}\n\n")
            
            f.write("## ğŸ¯ Attack Types Detected\n")
            for attack_type, ip_dict in self.analysis_results['attack_analysis'].items():
                if ip_dict:
                    f.write(f"- **{attack_type.replace('_', ' ').title()}**: {len(ip_dict)} IPs\n")
            
        
        print("\nâœ… Ù‡Ù…Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù†Ø¯:")
        print(f"  ğŸ“Š Excel: {excel_file}")
        print(f"  ğŸ“„ JSON: {json_file}")
        print("  ğŸ”’ Firewall Rules: iptables_rules.sh, htaccess_rules.txt, nginx_rules.conf")
        print("  ğŸ“ Ban Lists: ban_list.txt, critical_ips.txt")
        print("  ğŸ¤– Bot Report: bot_visits_report.txt")
        print("  ğŸ“‘ Summary: security_summary.md")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    import sys
    import argparse
    from datetime import timedelta
    
    # Parser Ø¨Ø±Ø§ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø· ÙØ±Ù…Ø§Ù†
    parser = argparse.ArgumentParser(
        description='Advanced Security Log Analyzer',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python analyzer.py access_log.gz --type wordpress --period 30
  python analyzer.py logs.zip --type opencart --excel --period 90
  python analyzer.py access.log --type general --all
        """
    )
    
    parser.add_argument('logfile', help='Path to log file (.gz, .zip, or plain text)')
    parser.add_argument('--type', '-t', 
                       choices=['wordpress', 'opencart', 'general'],
                       default='general',
                       help='Type of website (default: general)')
    parser.add_argument('--period', '-p',
                       type=int,
                       choices=[30, 60, 90, 180, 365, 0],
                       help='Time period in days (30/60/90/180/365/0 for all)')
    parser.add_argument('--interactive', '-i',
                       action='store_true',
                       help='Interactive mode for time period selection')
    parser.add_argument('--excel', '-e', 
                       action='store_true',
                       help='Generate Excel report')
    parser.add_argument('--json', '-j',
                       action='store_true',
                       help='Generate JSON report')
    parser.add_argument('--firewall', '-f',
                       action='store_true',
                       help='Generate firewall rules')
    parser.add_argument('--all', '-a',
                       action='store_true',
                       help='Generate all reports')
    parser.add_argument('--quiet', '-q',
                       action='store_true',
                       help='Minimal output')
    parser.add_argument('--timeline', '-tl',
                       action='store_true',
                       help='Generate bot timeline report')

    args = parser.parse_args()
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
    if not os.path.exists(args.logfile):
        print(f"âŒ ÙØ§ÛŒÙ„ {args.logfile} ÛŒØ§ÙØª Ù†Ø´Ø¯")
        sys.exit(1)
    
    # Header
    if not args.quiet:
        print("\n" + "="*80)
        print("ğŸ”’ Advanced Security Log Analyzer v2.0")
        print("="*80)
        print(f"ğŸ“ ÙØ§ÛŒÙ„: {args.logfile}")
        print(f"ğŸŒ Ù†ÙˆØ¹ Ø³Ø§ÛŒØª: {args.type}")
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¢Ù†Ø§Ù„Ø§ÛŒØ²Ø±
        analyzer = AdvancedSecurityAnalyzer(args.logfile, site_type=args.type)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
        days_limit = 0
        
        if args.interactive or (not args.period and not args.quiet):
            # Ø­Ø§Ù„Øª interactive: Ù†Ù…Ø§ÛŒØ´ Ù…Ù†Ùˆ
            days_limit = analyzer.select_time_period()
        elif args.period is not None:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù‚Ø¯Ø§Ø± Ø®Ø· ÙØ±Ù…Ø§Ù†
            days_limit = args.period
            period_names = {
                30: 'ÛŒÚ© Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                60: 'Ø¯Ùˆ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                90: 'Ø³Ù‡ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                180: 'Ø´Ø´ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                365: 'Ø¯ÙˆØ§Ø²Ø¯Ù‡ Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±',
                0: 'Ú©Ù„ Ù„Ø§Ú¯â€ŒÙ‡Ø§'
            }
            if not args.quiet:
                print(f"\nğŸ“… Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {period_names.get(days_limit, f'{days_limit} Ø±ÙˆØ²')}")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¨Ø§ ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ
        if not analyzer.load_logs(days_limit):
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ù„Ø§Ú¯")
            sys.exit(1)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù‡ Ø¢ÛŒØ§ Ù„Ø§Ú¯ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        if not analyzer.logs:
            print("\nâš ï¸ Ù‡ÛŒÚ† Ù„Ø§Ú¯ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            print("ğŸ’¡ Ù„Ø·ÙØ§Ù‹ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡ 'Ú©Ù„ Ù„Ø§Ú¯â€ŒÙ‡Ø§' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯")
            sys.exit(1)
        
        # Ø§Ø¯Ø§Ù…Ù‡ ØªØ­Ù„ÛŒÙ„...
        analyzer.generate_report()
        
        # ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ
        if args.all:
            analyzer.export_all_reports()
        else:
            if args.excel:
                analyzer.export_to_excel(analyzer.analysis_results)
            if args.json:
                analyzer.export_json_report()
            if args.firewall:
                analyzer.export_firewall_rules()
            if args.timeline:
                analyzer.generate_bot_timeline_report()

            # Ù‡Ù…ÛŒØ´Ù‡ Ù„ÛŒØ³Øª Ø¨Ù† Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†
            with open('ban_list.txt', 'w', encoding='utf-8') as f:
                f.write(f"# Suspicious IPs - {datetime.now()}\n")
                if days_limit > 0:
                    f.write(f"# Time Period: Last {days_limit} days\n")
                for ip in sorted(analyzer.suspicious_ips):
                    f.write(f"{ip}\n")
            print(f"\nâœ… Ù„ÛŒØ³Øª IP Ù‡Ø§ÛŒ Ù…Ø´Ú©ÙˆÚ© Ø¯Ø± ban_list.txt Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
        # Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
        if not args.quiet:
            print("\n" + "="*80)
            print("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:")
            print(f"  â€¢ {len(analyzer.suspicious_ips)} IP Ù…Ø´Ú©ÙˆÚ© Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
            print(f"  â€¢ {len(analyzer.critical_ips)} IP Ø¨Ø­Ø±Ø§Ù†ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ù‚Ø¯Ø§Ù… ÙÙˆØ±ÛŒ")
            print(f"  â€¢ {len(analyzer.analysis_results['bot_analysis']['fake'])} Ø¨Ø§Øª Ø¬Ø¹Ù„ÛŒ")
            
            if analyzer.critical_ips:
                print("\nâš ï¸ Ù‡Ø´Ø¯Ø§Ø±: IP Ù‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§ÛŒØ¯ ÙÙˆØ±Ø§Ù‹ Ø¨Ù† Ø´ÙˆÙ†Ø¯:")
                for ip in list(analyzer.critical_ips)[:5]:
                    print(f"  ğŸ”´ {ip}")
            
            print("\nğŸ’¡ Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§ØªØŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù„ØºÙˆ Ø´Ø¯")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        import traceback
        if not args.quiet:
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
